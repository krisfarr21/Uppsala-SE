{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AI Assignment 4 - Image Classifcation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_NT6C1eMC7S"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.datasets import cifar10  # dataset to be used for the assignment\n",
        "from keras.utils import np_utils\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from keras.layers import *\n",
        "from keras import backend as K\n",
        "import os\n",
        "from keras import regularizers\n",
        "import tensorflow.python.keras\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7vAyjPm06wv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "879fcd74-0680-4020-8fd2-983805ced81c"
      },
      "source": [
        "# TO CHECK IF ANY GPUS ARE AVAILABLE\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 11582550410570162829\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11345264640\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 16992684562107903143\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeYCI28_l8d6"
      },
      "source": [
        "def myGetModel(data):\n",
        "    weight_decay = 1e-4\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # 1\n",
        "    model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(weight_decay),\n",
        "                     input_shape=data.input_dim))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    # 2\n",
        "    model.add(layers.Conv2D(32, (3, 3), padding='same',\n",
        "                     kernel_regularizer=regularizers.l2(weight_decay),\n",
        "                     activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(layers.Dropout(0.2))\n",
        "\n",
        "    # 3\n",
        "    model.add(Conv2D(64, (3, 3), padding='same',\n",
        "                     kernel_regularizer=regularizers.l2(weight_decay),\n",
        "                     activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # 4\n",
        "    model.add(Conv2D(64, (3, 3), padding='same',\n",
        "                     kernel_regularizer=regularizers.l2(weight_decay),\n",
        "                     activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # 5\n",
        "    # model.add(Conv2D(128, (3, 3), padding='same',\n",
        "    #                  kernel_regularizer=regularizers.l2(weight_decay),\n",
        "    #                  activation='relu'))\n",
        "    # model.add(BatchNormalization())\n",
        "    model.add(Conv2D(64, (3, 3),\n",
        "                     padding='same',\n",
        "                    #  kernel_regularizer=regularizers.l2(weight_decay),\n",
        "                     activation='relu'))\n",
        "\n",
        "    # 6\n",
        "    # model.add(Conv2D(128, (3, 3),\n",
        "    #                  padding='same',\n",
        "    #                  kernel_regularizer=regularizers.l2(weight_decay),\n",
        "    #                  activation='relu'))\n",
        "\n",
        "    # NOTE: Some of the layers have been removed to reduce computational time.\n",
        "    # The obtained accuracy exceeds the par function without these layers. However,\n",
        "    # these can uncommented to further improve the results.\n",
        "    \n",
        "    model.add(Conv2D(64, (3, 3),\n",
        "                     padding='same',\n",
        "                    #  kernel_regularizer=regularizers.l2(weight_decay),\n",
        "                     activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(data.num_classes, activation='softmax'))\n",
        "\n",
        "    opt_rms = tensorflow.keras.optimizers.RMSprop(learning_rate=0.001,\n",
        "                                                  decay=1e-6)\n",
        "    \n",
        "    # NOTE: rms prop works much better \n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt_rms,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    #     initial_learning_rate=0.1,\n",
        "    #     decay_rate=1e-6,\n",
        "    #     decay_steps=0.1)\n",
        "    # optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)  # decay_steps=0.1)\n",
        "    # model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mN7iY3zpzSZ"
      },
      "source": [
        "# NOTE\n",
        "\n",
        "# from google.colab import drive # UNCOMMENT WHEN RUNNING ON GOOGLE COLAB\n",
        "# drive.mount('/content/gdrive') # UNCOMMENT WHEN RUNNING ON GOOGLE COLAB\n",
        "\n",
        "def myFitModel(model, data):\n",
        "    # checkpoint_path = os.path.dirname(os.path.realpath(__file__))  # UNCOMMENT WHEN RUNNING ON WINDOWS/MAC\n",
        "    checkpoint_path = \"/content/gdrive/MyDrive/Colab Notebooks\" # UNCOMMENT WHEN RUNNING ON GOOGLE COLAB\n",
        "    keras_callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                         patience=20, mode='min', min_delta=0.01,\n",
        "                                         restore_best_weights=True),\n",
        "        tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                           monitor='val_loss',\n",
        "                                           save_best_only=True, mode='min')\n",
        "    ]\n",
        "    # print('keras_callbacks', keras_callbacks)\n",
        "    \n",
        "    batch_size = 64\n",
        "    steps_per_epoch = len(data.x_train)//batch_size\n",
        "    validation_steps_per_epoch = len(data.x_valid)//batch_size\n",
        "    history = model.fit(data.x_train, data.y_train, \n",
        "                        batch_size=batch_size, epochs=10,\n",
        "                        steps_per_epoch=steps_per_epoch,\n",
        "                        validation_data=(data.x_valid, data.y_valid), \n",
        "                        validation_steps=validation_steps_per_epoch,\n",
        "                        verbose=2,\n",
        "                        callbacks=keras_callbacks)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JbCjGCgl8oh",
        "outputId": "b0d47017-7601-4c83-abf0-575f891ca3b1"
      },
      "source": [
        "def runImageClassification(seed=7):\n",
        "    # Fetch data. You may need to be connected to the internet the first time this is done.\n",
        "    # After the first time, it should be available in your system. On the off chance this\n",
        "    # is not the case on your system and you find yourself repeatedly downloading the data, \n",
        "    # you should change this code so you can load the data once and pass it to this function. \n",
        "    print(\"Preparing data...\")\n",
        "    data = CIFAR(seed)\n",
        "    print(data.y_valid_raw.shape)\n",
        "\n",
        "    # Create model\n",
        "    print(\"Creating model...\")\n",
        "    model = myGetModel(data)\n",
        "\n",
        "    # Fit model\n",
        "    print(\"Fitting model...\")\n",
        "    model = myFitModel(model, data)\n",
        "\n",
        "    # Evaluate on test data\n",
        "    print(\"Evaluating model...\")\n",
        "    score = model.evaluate(data.x_test, data.y_test, verbose=0)\n",
        "    print('Test accuracy:', score[1])\n",
        "\n",
        "runImageClassification()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing data...\n",
            "(9000, 1)\n",
            "Creating model...\n",
            "Fitting model...\n",
            "Epoch 1/10\n",
            "703/703 - 18s - loss: 1.6827 - accuracy: 0.4595 - val_loss: 1.2456 - val_accuracy: 0.5644\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Colab Notebooks/assets\n",
            "Epoch 2/10\n",
            "703/703 - 16s - loss: 1.0512 - accuracy: 0.6404 - val_loss: 0.9385 - val_accuracy: 0.6823\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Colab Notebooks/assets\n",
            "Epoch 3/10\n",
            "703/703 - 16s - loss: 0.8759 - accuracy: 0.7023 - val_loss: 0.8353 - val_accuracy: 0.7171\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Colab Notebooks/assets\n",
            "Epoch 4/10\n",
            "703/703 - 16s - loss: 0.7765 - accuracy: 0.7378 - val_loss: 1.5428 - val_accuracy: 0.5777\n",
            "Epoch 5/10\n",
            "703/703 - 16s - loss: 0.7113 - accuracy: 0.7615 - val_loss: 0.7411 - val_accuracy: 0.7584\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Colab Notebooks/assets\n",
            "Epoch 6/10\n",
            "703/703 - 16s - loss: 0.6635 - accuracy: 0.7791 - val_loss: 0.7580 - val_accuracy: 0.7517\n",
            "Epoch 7/10\n",
            "703/703 - 16s - loss: 0.6318 - accuracy: 0.7911 - val_loss: 0.6512 - val_accuracy: 0.7879\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Colab Notebooks/assets\n",
            "Epoch 8/10\n",
            "703/703 - 16s - loss: 0.6033 - accuracy: 0.8029 - val_loss: 0.6103 - val_accuracy: 0.8001\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Colab Notebooks/assets\n",
            "Epoch 9/10\n",
            "703/703 - 16s - loss: 0.5766 - accuracy: 0.8116 - val_loss: 0.6073 - val_accuracy: 0.8064\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Colab Notebooks/assets\n",
            "Epoch 10/10\n",
            "703/703 - 16s - loss: 0.5537 - accuracy: 0.8222 - val_loss: 0.6946 - val_accuracy: 0.7815\n",
            "Evaluating model...\n",
            "Test accuracy: 0.7718333601951599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMGF7BViWQSG"
      },
      "source": [
        "class CIFAR:\n",
        "    def __init__(self, seed=0):\n",
        "        # Get and split data\n",
        "        data = self.__getData(seed)\n",
        "        self.x_train_raw = data[0][0]\n",
        "        self.y_train_raw = data[0][1]\n",
        "        self.x_valid_raw = data[1][0]\n",
        "        self.y_valid_raw = data[1][1]\n",
        "        self.x_test_raw = data[2][0]\n",
        "        self.y_test_raw = data[2][1]\n",
        "        # Record input/output dimensions\n",
        "        self.num_classes=10\n",
        "        self.input_dim = self.x_train_raw.shape[1:]\n",
        "        # Convert data\n",
        "        self.y_train = np_utils.to_categorical(self.y_train_raw, self.num_classes)\n",
        "        self.y_valid = np_utils.to_categorical(self.y_valid_raw, self.num_classes)\n",
        "        self.y_test = np_utils.to_categorical(self.y_test_raw, self.num_classes)\n",
        "        self.x_train = self.x_train_raw.astype('float32')\n",
        "        self.x_valid = self.x_valid_raw.astype('float32')\n",
        "        self.x_test = self.x_test_raw.astype('float32')\n",
        "        self.x_train  /= 255\n",
        "        self.x_valid  /= 255\n",
        "        self.x_test /= 255\n",
        "        # Class names\n",
        "        self.class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                            'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "    def __getData(self,seed=0):\n",
        "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "        return self.__shuffleData(x_train,y_train,x_test,y_test,seed)\n",
        "    \n",
        "    def __shuffleData (self,x_train,y_train,x_test,y_test,seed=0):\n",
        "        tr_perc = .75\n",
        "        va_perc = .15\n",
        "        x=np.concatenate((x_train, x_test))\n",
        "        y=np.concatenate((y_train, y_test))\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(x)\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(y)\n",
        "        indices = np.random.permutation(len(x))\n",
        "        tr = round(len(x)*tr_perc)\n",
        "        va = round(len(x)*va_perc)\n",
        "        self.tr_indices = indices[0:tr]\n",
        "        self.va_indices = indices[tr:(tr+va)]\n",
        "        self.te_indices = indices[(tr+va):len(x)]\n",
        "        x_tr=x[self.tr_indices ,]\n",
        "        x_va=x[self.va_indices ,]\n",
        "        x_te=x[self.te_indices ,]\n",
        "        y_tr=y[self.tr_indices ,]\n",
        "        y_va=y[self.va_indices ,]\n",
        "        y_te=y[self.te_indices,]\n",
        "        return ((x_tr,y_tr),(x_va,y_va),(x_te,y_te))\n",
        "\n",
        "    # Print 25 random figures from the validation data\n",
        "    def showImages(self):\n",
        "        images=self.x_valid_raw\n",
        "        labels=self.y_valid_raw\n",
        "        class_names= ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                       'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        indices=np.random.randint(0, images.shape[0], 25)\n",
        "        for i in range(25):\n",
        "            plt.subplot(5, 5, i+1)\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "            plt.grid(False)\n",
        "            plt.imshow(images[indices[i]], cmap=plt.cm.binary)\n",
        "            # The CIFAR labels happen to be arrays, \n",
        "            # which is why we need the extra index\n",
        "            plt.xlabel(class_names[labels[indices[i]][0]])\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YZD81qUfW8S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
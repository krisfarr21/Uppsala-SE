{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2.2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iALmBVtm7qGE"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# to time models\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "#sklearn essentials\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, plot_confusion_matrix, plot_precision_recall_curve\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.utils._testing import ignore_warnings\n",
        "from sklearn.exceptions import FitFailedWarning, ConvergenceWarning"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7hFStE179dk"
      },
      "source": [
        "# Fetch_20newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCsjdUUC7vpn"
      },
      "source": [
        "# Loading Data!\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Splitting Data\n",
        "newsgroup_info = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "ng_x_train, ng_x_test, ng_y_train, ng_y_test = train_test_split(newsgroup_info.data, newsgroup_info.target, \n",
        "                                                        test_size=0.2, random_state=0)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hul_n7hG8GJA"
      },
      "source": [
        "## Probabilistic Model - Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrOmVpg972tG"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rfc_ng_model = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(stop_words='english', ngram_range=(1, 2))),\n",
        "    ('tfidf', TfidfTransformer(use_idf=True, norm='l2')),\n",
        "    ('classifier', RandomForestClassifier(max_depth=10)) # to reduce time...\n",
        "                    ], verbose=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0waUioOY8MVV",
        "outputId": "1836d6df-4451-4058-a2ed-c025ea9ff3cb"
      },
      "source": [
        "print(\"Random Forest Classifier\")\n",
        "print()\n",
        "start = timer()\n",
        "rfc_ng_model.fit(ng_x_train, ng_y_train)\n",
        "end = timer()\n",
        "print()\n",
        "print(\"Time taken in seconds to fit the model!\", round(end-start, 5))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest Classifier\n",
            "\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   9.2s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   1.0s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=   4.2s\n",
            "\n",
            "Time taken in seconds to fit the model! 14.40766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkgtpKdY8Nw2",
        "outputId": "812ee74a-a90c-42f5-ccb4-9f8954213fdb"
      },
      "source": [
        "rfc_ng_y_pred = rfc_ng_model.predict(ng_x_test)\n",
        "print(classification_report(ng_y_test, rfc_ng_y_pred, target_names=newsgroup_info.target_names, zero_division='warn'))\n",
        "print('\\n\\n')\n",
        "print(\"Using random forest classifier yieled an accuracy of --->\", rfc_ng_model.score(ng_x_test, ng_y_test))\n",
        "print(\"Using random forest classifier yieled a micro f1-score of --->\", f1_score(ng_y_test, rfc_ng_y_pred, average='micro'))\n",
        "print(\"Using random forest classifier yieled a macro f1-score of --->\", f1_score(ng_y_test, rfc_ng_y_pred, average='macro'))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       1.00      0.07      0.14       163\n",
            "           comp.graphics       0.55      0.54      0.55       190\n",
            " comp.os.ms-windows.misc       0.60      0.41      0.49       200\n",
            "comp.sys.ibm.pc.hardware       0.57      0.60      0.58       196\n",
            "   comp.sys.mac.hardware       0.77      0.58      0.66       201\n",
            "          comp.windows.x       0.66      0.63      0.64       198\n",
            "            misc.forsale       0.78      0.61      0.68       206\n",
            "               rec.autos       0.14      0.66      0.23       177\n",
            "         rec.motorcycles       0.62      0.58      0.60       189\n",
            "      rec.sport.baseball       0.38      0.70      0.50       171\n",
            "        rec.sport.hockey       0.86      0.67      0.75       233\n",
            "               sci.crypt       0.71      0.66      0.68       190\n",
            "         sci.electronics       0.60      0.23      0.33       207\n",
            "                 sci.med       0.82      0.57      0.67       203\n",
            "               sci.space       0.75      0.58      0.65       191\n",
            "  soc.religion.christian       0.43      0.77      0.55       198\n",
            "      talk.politics.guns       0.64      0.55      0.59       155\n",
            "   talk.politics.mideast       0.71      0.70      0.70       196\n",
            "      talk.politics.misc       1.00      0.06      0.11       170\n",
            "      talk.religion.misc       0.00      0.00      0.00       136\n",
            "\n",
            "                accuracy                           0.52      3770\n",
            "               macro avg       0.63      0.51      0.51      3770\n",
            "            weighted avg       0.64      0.52      0.52      3770\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Using random forest classifier yieled an accuracy of ---> 0.5209549071618037\n",
            "Using random forest classifier yieled a micro f1-score of ---> 0.5209549071618037\n",
            "Using random forest classifier yieled a macro f1-score of ---> 0.5053645825468998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Mb1FBEb9rww"
      },
      "source": [
        "### Random Forest Classifier - Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb8nlfmR9xSL"
      },
      "source": [
        "parameters_rfc_ng = {\n",
        "    # 'vectorizer__stop_words' : ('english', None),\n",
        "    # 'vectorizer__ngram_range' : ((1, 1), (1, 2)), \n",
        "    # 'vectorizer__max_df' : (0.5, 0.75, 1.0),\n",
        "    # 'vectorizer__max_features' : (5000, 10000, 50000),\n",
        "    # 'tfidf__use_idf': (True, False),\n",
        "    # 'tfidf__norm' : ('l1', 'l2'),\n",
        "    'classifier__n_estimators' : (100, 200, 300),\n",
        "    'classifier__max_depth' : (25, 50, 100),\n",
        "    'classifier__bootstrap' : (True, False)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Jz5i2qa93bA",
        "outputId": "59c9fd7b-e91e-44c7-b9fe-2ef3a745f83f"
      },
      "source": [
        "grid_search_rfc_ng = GridSearchCV(rfc_ng_model, parameters_rfc_ng, cv=2, n_jobs=1)\n",
        "start = timer()\n",
        "grid_search_rfc_ng = grid_search_rfc_ng.fit(ng_x_train, ng_y_train)\n",
        "end = timer()\n",
        "print(\"Time taken to execute Grid search for Random Forest Classifieri!\", round(end-start)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.7s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=   7.7s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.4s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=   7.8s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.5s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  15.3s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.3s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  16.3s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.6s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  22.4s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.4s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  20.6s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.5s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  20.5s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.3s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  23.9s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.5s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  45.1s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.5s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  46.3s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.5s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 1.0min\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.4s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.3s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 1.1min\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.6s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  53.3s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.3s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  57.0s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.5s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 1.7min\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.4s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 1.9min\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.6s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 2.6min\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   5.1s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.3s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 2.6min\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.7s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  10.4s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.4s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  11.0s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.5s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  19.3s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.6s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.3s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  21.0s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.6s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  30.6s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.3s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  32.6s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.6s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.3s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  31.6s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.3s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.3s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  33.6s\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.6s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.3s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 1.1min\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.5s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 1.2min\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.7s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 1.6min\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.4s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 1.7min\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.6s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.3s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 1.4min\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.5s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 1.5min\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.5s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 2.7min\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.4s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 2.9min\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.6s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 4.0min\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.5s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 4.3min\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   9.1s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.8s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=13.7min\n",
            "Time taken to execute Grid search for Multinomial Naive bayes! 3700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o44hRRmZ96qv",
        "outputId": "24900243-19ea-46fd-8a11-20b99706a38d"
      },
      "source": [
        "print(\"Best Score for random forest classifier: \", grid_search_rfc_ng.best_score_)\n",
        "print()\n",
        "print(\"Best parameters:\", grid_search_rfc_ng.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Score for Multinomial Naive Bayes:  0.6671530910055719\n",
            "\n",
            "Best parameters: {'classifier__bootstrap': False, 'classifier__max_depth': 100, 'classifier__n_estimators': 300}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTqnlLKW-Cin",
        "outputId": "048d5cba-46e7-4578-9f19-cd6d123af1d8"
      },
      "source": [
        "grid_search_rfc_ng_y_pred = grid_search_rfc_ng.predict(ng_x_test)\n",
        "print('SCORES WITH RANDOM FOREST CLASSIFIER AFTER GRID SEARCH')\n",
        "print('\\n\\n')\n",
        "print(classification_report(ng_y_test, grid_search_rfc_ng_y_pred, target_names=newsgroup_info.target_names, zero_division='warn'))\n",
        "print('\\n\\n')\n",
        "print(\"Using random forest classifier yieled an accuracy of --->\", grid_search_rfc_ng.score(ng_x_test, ng_y_test))\n",
        "print(\"Using random forest classifier a micro f1-score of --->\", f1_score(ng_y_test, grid_search_rfc_ng_y_pred, average='micro'))\n",
        "print(\"Using random forest classifier a macro f1-score of --->\", f1_score(ng_y_test, grid_search_rfc_ng_y_pred, average='macro'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SCORES WITH RANDOM FOREST CLASSIFIER AFTER GRID SEARCH\n",
            "\n",
            "\n",
            "\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.73      0.42      0.53       163\n",
            "           comp.graphics       0.62      0.64      0.63       190\n",
            " comp.os.ms-windows.misc       0.64      0.65      0.64       200\n",
            "comp.sys.ibm.pc.hardware       0.65      0.65      0.65       196\n",
            "   comp.sys.mac.hardware       0.82      0.67      0.74       201\n",
            "          comp.windows.x       0.80      0.77      0.78       198\n",
            "            misc.forsale       0.79      0.77      0.78       206\n",
            "               rec.autos       0.56      0.72      0.63       177\n",
            "         rec.motorcycles       0.32      0.78      0.45       189\n",
            "      rec.sport.baseball       0.83      0.81      0.82       171\n",
            "        rec.sport.hockey       0.85      0.85      0.85       233\n",
            "               sci.crypt       0.83      0.72      0.77       190\n",
            "         sci.electronics       0.78      0.57      0.66       207\n",
            "                 sci.med       0.83      0.77      0.80       203\n",
            "               sci.space       0.83      0.77      0.80       191\n",
            "  soc.religion.christian       0.59      0.82      0.69       198\n",
            "      talk.politics.guns       0.57      0.74      0.64       155\n",
            "   talk.politics.mideast       0.90      0.83      0.86       196\n",
            "      talk.politics.misc       0.83      0.37      0.51       170\n",
            "      talk.religion.misc       0.78      0.10      0.18       136\n",
            "\n",
            "                accuracy                           0.68      3770\n",
            "               macro avg       0.73      0.67      0.67      3770\n",
            "            weighted avg       0.73      0.68      0.68      3770\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Using multinomial naive bayes yieled an accuracy of ---> 0.6840848806366048\n",
            "Using multinomial naive bayes a micro f1-score of ---> 0.6840848806366048\n",
            "Using multinomial naive bayes a macro f1-score of ---> 0.6712330750303989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amDT3GZJ87tS"
      },
      "source": [
        "## Non-Probabilistic Model - Linear Support Vector Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn6kkSkt8zoq"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Linear Support Vector Classification - this should have worked better....right?\n",
        "\n",
        "lsvc_ng_model = Pipeline([\n",
        "                    ('vectorizer', CountVectorizer(stop_words='english', ngram_range=(1, 2))),\n",
        "                    ('tfidf', TfidfTransformer(use_idf=True, norm='l2')),\n",
        "                    ('classifier', LinearSVC())  \n",
        "                    ], verbose=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5bQKWLE9b8W",
        "outputId": "4405ca3f-0a98-4963-a8dd-9a59f4012808"
      },
      "source": [
        "print(\"Linear Support Vector Classifier\")\n",
        "print()\n",
        "start = timer()\n",
        "lsvc_ng_model.fit(ng_x_train, ng_y_train)\n",
        "end = timer()\n",
        "print()\n",
        "print('Time taken in seconds to fit the model!', round(end-start, 5))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Support Vector Classifier\n",
            "\n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   8.6s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   1.0s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  10.8s\n",
            "\n",
            "Time taken in seconds to fit the model! 20.41869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfew1tUj9F5v",
        "outputId": "ce7383e4-fd41-4d45-cafd-aff6c42a1757"
      },
      "source": [
        "lsvc_ng_y_pred = lsvc_ng_model.predict(ng_x_test)\n",
        "print(classification_report(ng_y_test, lsvc_ng_y_pred, target_names=newsgroup_info.target_names, zero_division='warn'))\n",
        "print('\\n\\n')\n",
        "print(\"Using Linear Support Vector Classification yieled an accuracy of --->\", lsvc_ng_model.score(ng_x_test, ng_y_test))\n",
        "print(\"Using support vector classification yielded a micro f1-score of --->\", f1_score(ng_y_test, lsvc_ng_y_pred, average='micro'))\n",
        "print(\"Using support vector classification yielded a micro f1-score of --->\", f1_score(ng_y_test, lsvc_ng_y_pred, average='macro'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.72      0.64      0.68       163\n",
            "           comp.graphics       0.71      0.75      0.73       190\n",
            " comp.os.ms-windows.misc       0.73      0.69      0.71       200\n",
            "comp.sys.ibm.pc.hardware       0.72      0.69      0.71       196\n",
            "   comp.sys.mac.hardware       0.81      0.74      0.78       201\n",
            "          comp.windows.x       0.83      0.83      0.83       198\n",
            "            misc.forsale       0.80      0.83      0.82       206\n",
            "               rec.autos       0.49      0.83      0.62       177\n",
            "         rec.motorcycles       0.86      0.80      0.83       189\n",
            "      rec.sport.baseball       0.87      0.87      0.87       171\n",
            "        rec.sport.hockey       0.95      0.89      0.92       233\n",
            "               sci.crypt       0.87      0.79      0.83       190\n",
            "         sci.electronics       0.77      0.76      0.77       207\n",
            "                 sci.med       0.86      0.85      0.85       203\n",
            "               sci.space       0.82      0.82      0.82       191\n",
            "  soc.religion.christian       0.70      0.84      0.77       198\n",
            "      talk.politics.guns       0.69      0.75      0.72       155\n",
            "   talk.politics.mideast       0.88      0.86      0.87       196\n",
            "      talk.politics.misc       0.73      0.62      0.67       170\n",
            "      talk.religion.misc       0.71      0.38      0.50       136\n",
            "\n",
            "                accuracy                           0.77      3770\n",
            "               macro avg       0.78      0.76      0.76      3770\n",
            "            weighted avg       0.78      0.77      0.77      3770\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Using Linear Support Vector Classification yieled an accuracy of ---> 0.7710875331564987\n",
            "Using support vector classification yielded a micro f1-score of ---> 0.7710875331564987\n",
            "Using support vector classification yielded a micro f1-score of ---> 0.7636511602855547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEyps2JH-Np5"
      },
      "source": [
        "### Linear Support Vector Classification - Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNi6OjTu9ZBo"
      },
      "source": [
        "parameters_lsvc_ng = {\n",
        "    # 'vectorizer__stop_words' : ('english', None),\n",
        "    # 'vectorizer__ngram_range' : ((1, 1), (1, 2)),\n",
        "    # 'vectorizer__max_df' : (0.5, 0.75, 1.0),\n",
        "    # 'vectorizer__max_features' : (None, 5000, 10000, 50000),\n",
        "    # 'tfidf__use_idf': (True, False),\n",
        "    # 'tfidf__norm' : ('l1', 'l2'),\n",
        "    # 'classifier__penalty' : ('l1', 'l2'),\n",
        "    'classifier__loss' : ('hinge', 'squared_hinge'),\n",
        "    # 'classifier__dual' : (False, True),\n",
        "    'classifier__multi_class' : ('ovr', 'crammer_singer'),\n",
        "    'classifier__max_iter' : (1000, 2000, 5000)\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3ZoqSZT-WLU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb095a6-0e01-4d13-be69-d5237ae9159a"
      },
      "source": [
        "grid_search_lsvc_ng = GridSearchCV(lsvc_ng_model, parameters_lsvc_ng, cv=2, n_jobs=1, verbose=3)\n",
        "\n",
        "start = timer()\n",
        "grid_search_lsvc_ng = grid_search_lsvc_ng.fit(ng_x_train, ng_y_train)\n",
        "end = timer()\n",
        "print(\"Time taken to execute Grid search for Support Vector Classification!\", round(end-start))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
            "[CV] classifier__loss=hinge, classifier__max_iter=1000, classifier__multi_class=ovr \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.4s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  19.7s\n",
            "[CV]  classifier__loss=hinge, classifier__max_iter=1000, classifier__multi_class=ovr, score=0.736, total=  26.6s\n",
            "[CV] classifier__loss=hinge, classifier__max_iter=1000, classifier__multi_class=ovr \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   26.6s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.2s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  16.6s\n",
            "[CV]  classifier__loss=hinge, classifier__max_iter=1000, classifier__multi_class=ovr, score=0.738, total=  23.1s\n",
            "[CV] classifier__loss=hinge, classifier__max_iter=1000, classifier__multi_class=crammer_singer \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   49.8s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.3s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 1.2min\n",
            "[CV]  classifier__loss=hinge, classifier__max_iter=1000, classifier__multi_class=crammer_singer, score=0.743, total= 1.3min\n",
            "[CV] classifier__loss=hinge, classifier__max_iter=1000, classifier__multi_class=crammer_singer \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.2s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 2.3min\n",
            "[CV]  classifier__loss=hinge, classifier__max_iter=1000, classifier__multi_class=crammer_singer, score=0.739, total= 2.4min\n",
            "[CV] classifier__loss=hinge, classifier__max_iter=2000, classifier__multi_class=ovr \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.4s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.3s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  20.7s\n",
            "[CV]  classifier__loss=hinge, classifier__max_iter=2000, classifier__multi_class=ovr, score=0.734, total=  27.4s\n",
            "[CV] classifier__loss=hinge, classifier__max_iter=2000, classifier__multi_class=ovr \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.2s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  16.4s\n",
            "[CV]  classifier__loss=hinge, classifier__max_iter=2000, classifier__multi_class=ovr, score=0.738, total=  22.9s\n",
            "[CV] classifier__loss=hinge, classifier__max_iter=2000, classifier__multi_class=crammer_singer \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.4s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  53.1s\n",
            "[CV]  classifier__loss=hinge, classifier__max_iter=2000, classifier__multi_class=crammer_singer, score=0.742, total=  59.9s\n",
            "[CV] classifier__loss=hinge, classifier__max_iter=2000, classifier__multi_class=crammer_singer \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.3s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 2.2min\n",
            "[CV]  classifier__loss=hinge, classifier__max_iter=2000, classifier__multi_class=crammer_singer, score=0.739, total= 2.3min\n",
            "[CV] classifier__loss=hinge, classifier__max_iter=5000, classifier__multi_class=ovr \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.3s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  23.6s\n",
            "[CV]  classifier__loss=hinge, classifier__max_iter=5000, classifier__multi_class=ovr, score=0.736, total=  30.3s\n",
            "[CV] classifier__loss=hinge, classifier__max_iter=5000, classifier__multi_class=ovr \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.2s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  16.8s\n",
            "[CV]  classifier__loss=hinge, classifier__max_iter=5000, classifier__multi_class=ovr, score=0.740, total=  23.4s\n",
            "[CV] classifier__loss=hinge, classifier__max_iter=5000, classifier__multi_class=crammer_singer \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.3s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.3s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 1.4min\n",
            "[CV]  classifier__loss=hinge, classifier__max_iter=5000, classifier__multi_class=crammer_singer, score=0.743, total= 1.5min\n",
            "[CV] classifier__loss=hinge, classifier__max_iter=5000, classifier__multi_class=crammer_singer \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.2s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 2.9min\n",
            "[CV]  classifier__loss=hinge, classifier__max_iter=5000, classifier__multi_class=crammer_singer, score=0.739, total= 3.0min\n",
            "[CV] classifier__loss=squared_hinge, classifier__max_iter=1000, classifier__multi_class=ovr \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.5s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=   4.4s\n",
            "[CV]  classifier__loss=squared_hinge, classifier__max_iter=1000, classifier__multi_class=ovr, score=0.742, total=  11.2s\n",
            "[CV] classifier__loss=squared_hinge, classifier__max_iter=1000, classifier__multi_class=ovr \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.3s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=   4.5s\n",
            "[CV]  classifier__loss=squared_hinge, classifier__max_iter=1000, classifier__multi_class=ovr, score=0.739, total=  11.2s\n",
            "[CV] classifier__loss=squared_hinge, classifier__max_iter=1000, classifier__multi_class=crammer_singer \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.4s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 1.2min\n",
            "[CV]  classifier__loss=squared_hinge, classifier__max_iter=1000, classifier__multi_class=crammer_singer, score=0.743, total= 1.4min\n",
            "[CV] classifier__loss=squared_hinge, classifier__max_iter=1000, classifier__multi_class=crammer_singer \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.2s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 2.2min\n",
            "[CV]  classifier__loss=squared_hinge, classifier__max_iter=1000, classifier__multi_class=crammer_singer, score=0.738, total= 2.3min\n",
            "[CV] classifier__loss=squared_hinge, classifier__max_iter=2000, classifier__multi_class=ovr \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.4s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.3s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=   4.4s\n",
            "[CV]  classifier__loss=squared_hinge, classifier__max_iter=2000, classifier__multi_class=ovr, score=0.742, total=  11.2s\n",
            "[CV] classifier__loss=squared_hinge, classifier__max_iter=2000, classifier__multi_class=ovr \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.3s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=   4.5s\n",
            "[CV]  classifier__loss=squared_hinge, classifier__max_iter=2000, classifier__multi_class=ovr, score=0.739, total=  11.2s\n",
            "[CV] classifier__loss=squared_hinge, classifier__max_iter=2000, classifier__multi_class=crammer_singer \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.4s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  58.0s\n",
            "[CV]  classifier__loss=squared_hinge, classifier__max_iter=2000, classifier__multi_class=crammer_singer, score=0.742, total= 1.1min\n",
            "[CV] classifier__loss=squared_hinge, classifier__max_iter=2000, classifier__multi_class=crammer_singer \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.2s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 2.0min\n",
            "[CV]  classifier__loss=squared_hinge, classifier__max_iter=2000, classifier__multi_class=crammer_singer, score=0.739, total= 2.1min\n",
            "[CV] classifier__loss=squared_hinge, classifier__max_iter=5000, classifier__multi_class=ovr \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.4s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.3s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=   4.5s\n",
            "[CV]  classifier__loss=squared_hinge, classifier__max_iter=5000, classifier__multi_class=ovr, score=0.742, total=  11.3s\n",
            "[CV] classifier__loss=squared_hinge, classifier__max_iter=5000, classifier__multi_class=ovr \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.2s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=   4.3s\n",
            "[CV]  classifier__loss=squared_hinge, classifier__max_iter=5000, classifier__multi_class=ovr, score=0.739, total=  10.9s\n",
            "[CV] classifier__loss=squared_hinge, classifier__max_iter=5000, classifier__multi_class=crammer_singer \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.4s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total=  42.9s\n",
            "[CV]  classifier__loss=squared_hinge, classifier__max_iter=5000, classifier__multi_class=crammer_singer, score=0.743, total=  49.7s\n",
            "[CV] classifier__loss=squared_hinge, classifier__max_iter=5000, classifier__multi_class=crammer_singer \n",
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   4.2s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.4s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 2.8min\n",
            "[CV]  classifier__loss=squared_hinge, classifier__max_iter=5000, classifier__multi_class=crammer_singer, score=0.739, total= 2.9min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed: 25.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ........ (step 1 of 3) Processing vectorizer, total=   8.4s\n",
            "[Pipeline] ............. (step 2 of 3) Processing tfidf, total=   0.8s\n",
            "[Pipeline] ........ (step 3 of 3) Processing classifier, total= 5.8min\n",
            "Time taken to execute Grid search for Support Vector Classification! 1897\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jMKF1vO-aLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22ccc643-937b-4c64-cdcb-9f513f08ba6a"
      },
      "source": [
        "print(\"Best Score:\", grid_search_lsvc_ng.best_score_)\n",
        "print()\n",
        "print(\"Best Parameters\", grid_search_lsvc_ng.best_params_)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Score: 0.7409127089413637\n",
            "\n",
            "Best Parameters {'classifier__loss': 'hinge', 'classifier__max_iter': 1000, 'classifier__multi_class': 'crammer_singer'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6Kzn3aT-fe0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eed6441a-2364-4735-983e-0122cb8591c4"
      },
      "source": [
        "grid_search_lsvc_ng_y_pred = grid_search_lsvc_ng.predict(ng_x_test)\n",
        "print('SCORES WITH LINEAR SUPPORT VECTOR CLASSIFICATION AFTER GRID SEARCH')\n",
        "print('\\n\\n')\n",
        "print(classification_report(ng_y_test, grid_search_lsvc_ng_y_pred, target_names=newsgroup_info.target_names, zero_division='warn'))\n",
        "print('\\n\\n')\n",
        "print(\"Using linear support vector classification yieled an accuracy of --->\", grid_search_lsvc_ng.score(ng_x_test, ng_y_test))\n",
        "print(\"Using linear support vector classification a micro f1-score of --->\", f1_score(ng_y_test, grid_search_lsvc_ng_y_pred, average='micro'))\n",
        "print(\"Using linear support vector classification a macro f1-score of --->\", f1_score(ng_y_test, grid_search_lsvc_ng_y_pred, average='macro'))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SCORES WITH RANDOM FOREST CLASSIFIER AFTER GRID SEARCH\n",
            "\n",
            "\n",
            "\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.73      0.66      0.69       163\n",
            "           comp.graphics       0.71      0.75      0.73       190\n",
            " comp.os.ms-windows.misc       0.72      0.68      0.70       200\n",
            "comp.sys.ibm.pc.hardware       0.74      0.69      0.71       196\n",
            "   comp.sys.mac.hardware       0.81      0.74      0.78       201\n",
            "          comp.windows.x       0.83      0.84      0.84       198\n",
            "            misc.forsale       0.81      0.83      0.82       206\n",
            "               rec.autos       0.78      0.80      0.79       177\n",
            "         rec.motorcycles       0.85      0.80      0.83       189\n",
            "      rec.sport.baseball       0.84      0.87      0.86       171\n",
            "        rec.sport.hockey       0.95      0.88      0.91       233\n",
            "               sci.crypt       0.89      0.79      0.84       190\n",
            "         sci.electronics       0.79      0.78      0.79       207\n",
            "                 sci.med       0.57      0.90      0.70       203\n",
            "               sci.space       0.83      0.82      0.82       191\n",
            "  soc.religion.christian       0.71      0.82      0.76       198\n",
            "      talk.politics.guns       0.71      0.76      0.73       155\n",
            "   talk.politics.mideast       0.89      0.85      0.87       196\n",
            "      talk.politics.misc       0.74      0.65      0.69       170\n",
            "      talk.religion.misc       0.67      0.43      0.52       136\n",
            "\n",
            "                accuracy                           0.78      3770\n",
            "               macro avg       0.78      0.77      0.77      3770\n",
            "            weighted avg       0.78      0.78      0.78      3770\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Using multinomial naive bayes yieled an accuracy of ---> 0.7753315649867374\n",
            "Using multinomial naive bayes a micro f1-score of ---> 0.7753315649867373\n",
            "Using multinomial naive bayes a macro f1-score of ---> 0.7690140319781549\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SCLKIfe_KRM"
      },
      "source": [
        "# Reatures Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk0QEaw5_n-B"
      },
      "source": [
        "## Loading and Parsing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ntc5Xvgp_N5R",
        "outputId": "46f078bb-63df-4fe2-8063-304e7b9f6cbb"
      },
      "source": [
        "# Loading Data\n",
        "\n",
        "!wget -N http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-13 17:49:29--  http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz\n",
            "Resolving kdd.ics.uci.edu (kdd.ics.uci.edu)... 128.195.1.86\n",
            "Connecting to kdd.ics.uci.edu (kdd.ics.uci.edu)|128.195.1.86|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8150596 (7.8M) [application/x-gzip]\n",
            "Saving to: ‘reuters21578.tar.gz’\n",
            "\n",
            "reuters21578.tar.gz 100%[===================>]   7.77M  3.02MB/s    in 2.6s    \n",
            "\n",
            "2021-06-13 17:49:31 (3.02 MB/s) - ‘reuters21578.tar.gz’ saved [8150596/8150596]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6nyfFHj_SOd"
      },
      "source": [
        "# Parsing Data\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from collections import defaultdict \n",
        "\n",
        "xml_tags = [ 'title', 'date', 'dateline']\n",
        "\n",
        "attribute_values = ['cgisplit', 'lewissplit', 'newid', 'oldid', 'topics']\n",
        "\n",
        "d_xml_tags = ['topics', 'places', 'orgs', 'exchanges', 'companies']\n",
        "\n",
        "info_dict = defaultdict(lambda: defaultdict(lambda: '') )\n",
        "\n",
        "# counter used to name different documents (title can be used instead, working \n",
        "# with this for now)\n",
        "\n",
        "counter = 0 \n",
        "\n",
        "files = !tar xvzf reuters21578.tar.gz\n",
        "\n",
        "for file in files[9:]:\n",
        "    with open(file, 'rb') as f2:\n",
        "        if file.endswith('.sgm'):\n",
        "            # file = file[6:9]\n",
        "            filecontent = f2.readlines()\n",
        "            filecontent = b\"\".join(filecontent)\n",
        "\n",
        "            # BS whole xml file\n",
        "            soup = bs(filecontent, 'lxml')\n",
        "\n",
        "            # Reuters are used to delimit documents....\n",
        "            documents = soup.find_all('reuters')\n",
        "\n",
        "            # Three different XML extracts...\n",
        "            # (i) parsing date tags... e.g. <date> </date>\n",
        "            for doc in documents:\n",
        "                counter += 1\n",
        "                for tag1 in xml_tags:\n",
        "                    try:\n",
        "                        # (doc.find('title').text)\n",
        "                        to_add = doc.find(tag1).text.strip()\n",
        "                        info_dict[counter][tag1] += to_add\n",
        "\n",
        "                    except AttributeError:\n",
        "                        info_dict[counter][tag1] += str(None)\n",
        "\n",
        "            # # (ii) parsing attribute values e.g. cgisplit='example'\n",
        "                # for attribute in attribute_values:\n",
        "                #     try:\n",
        "                #         to_add = doc.get(attribute)\n",
        "                #         info_dict[counter][attribute] += (to_add)\n",
        "                #     except:\n",
        "                #         info_dict[counter][attribute] += str(None)\n",
        "\n",
        "            # (iii) parsing child tags, i.e. find in find_all.. (hierarchical tags)\n",
        "                for tag2 in d_xml_tags:\n",
        "                    to_concatenate = [elem.text for elem in doc.find(tag2).findAll('d')]\n",
        "                    if len(to_concatenate) > 0:\n",
        "                        info_dict[counter][tag2] += ' '.join(to_concatenate)\n",
        "                    else:\n",
        "                        if tag2 != 'topics':\n",
        "                            info_dict[counter][tag2] += str(None)\n",
        "\n",
        "            # finally extracting texts..\n",
        "                text_tag = doc.find('text')\n",
        "                text = text_tag.text.strip()\n",
        "                \n",
        "                try:\n",
        "                    text = text.replace(text_tag.title.text, '')\n",
        "                except:\n",
        "                    pass\n",
        "                try:\n",
        "                    text = text.replace(text_tag.dateline.text, '')\n",
        "                except:\n",
        "                    pass\n",
        "                text = text.replace('\\n', ' ')\n",
        "                info_dict[counter]['text'] += text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfWZexmGAUB0"
      },
      "source": [
        "# Appending the raw texts and labels (places) to lists\n",
        "X_raw = [] \n",
        "labels = []\n",
        "\n",
        "for key, value in info_dict.items():\n",
        "    labels.append(value['places'].split())\n",
        "    X_raw.append(value['text'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLOFAMp_7yE"
      },
      "source": [
        "## Probabilsitc Model - Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lslmP0Eg_sdQ"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X_raw)\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "y = mlb.fit_transform(labels)\n",
        "\n",
        "rfc_reut_model = RandomForestClassifier(max_depth=10)\n",
        "\n",
        "reut_x_train, reut_x_test, reut_y_train, reut_y_test = train_test_split(X, y, \n",
        "                                                        test_size=0.33, random_state=0)\n",
        "\n",
        "# https://github.com/davidsbatista/text-classification/issues/1"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmf0imt_ALGl",
        "outputId": "63cfe562-25fa-4dc2-e620-7fa77e90431c"
      },
      "source": [
        "print(\"Random Forest Classifier\")\n",
        "print()\n",
        "start = timer()\n",
        "rfc_reut_model.fit(reut_x_train, reut_y_train)\n",
        "end = timer()\n",
        "print()\n",
        "print(\"Time taken in seconds to fit the model!\", round(end-start, 5))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest Classifier\n",
            "\n",
            "\n",
            "Time taken in seconds to fit the model! 30.61041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjKutuVyB5ge",
        "outputId": "ccd867c9-2fcd-4ce3-bfa9-13ee6f4dd8d2"
      },
      "source": [
        "rfc_reut_y_pred = rfc_reut_model.predict(reut_x_test)\n",
        "print(classification_report(reut_y_test, rfc_reut_y_pred, target_names=mlb.classes_, zero_division='warn'))\n",
        "print('\\n\\n')\n",
        "print(\"Usireut random forest classifier yieled an accuracy of --->\", rfc_reut_model.score(reut_x_test, reut_y_test))\n",
        "print(\"Usireut random forest classifier yieled a micro f1-score of --->\", f1_score(reut_y_test, rfc_reut_y_pred, average='micro'))\n",
        "print(\"Usireut random forest classifier yieled a macro f1-score of --->\", f1_score(reut_y_test, rfc_reut_y_pred, average='macro'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "               None       1.00      0.01      0.02       884\n",
            "        afghanistan       0.00      0.00      0.00         1\n",
            "            algeria       0.00      0.00      0.00        11\n",
            "             angola       0.00      0.00      0.00         1\n",
            "            antigua       0.00      0.00      0.00         0\n",
            "          argentina       0.00      0.00      0.00        28\n",
            "              aruba       0.00      0.00      0.00         1\n",
            "          australia       0.00      0.00      0.00        85\n",
            "            austria       0.00      0.00      0.00         6\n",
            "            bahamas       0.00      0.00      0.00         0\n",
            "            bahrain       0.00      0.00      0.00        13\n",
            "         bangladesh       0.00      0.00      0.00        11\n",
            "           barbados       0.00      0.00      0.00         0\n",
            "            belgium       0.00      0.00      0.00        69\n",
            "              benin       0.00      0.00      0.00         0\n",
            "            bermuda       0.00      0.00      0.00         2\n",
            "             bhutan       0.00      0.00      0.00         0\n",
            "            bolivia       0.00      0.00      0.00         8\n",
            "           botswana       0.00      0.00      0.00         1\n",
            "             brazil       0.00      0.00      0.00       106\n",
            "             brunei       0.00      0.00      0.00         1\n",
            "           bulgaria       0.00      0.00      0.00         0\n",
            "       burkina-faso       0.00      0.00      0.00         0\n",
            "              burma       0.00      0.00      0.00         1\n",
            "           cameroon       0.00      0.00      0.00         1\n",
            "             canada       0.00      0.00      0.00       385\n",
            "     cayman-islands       0.00      0.00      0.00         0\n",
            "               chad       0.00      0.00      0.00         1\n",
            "              chile       0.00      0.00      0.00         4\n",
            "              china       0.00      0.00      0.00        65\n",
            "           colombia       0.00      0.00      0.00        12\n",
            "              congo       0.00      0.00      0.00         2\n",
            "         costa-rica       0.00      0.00      0.00         8\n",
            "               cuba       0.00      0.00      0.00         9\n",
            "             cyprus       0.00      0.00      0.00         7\n",
            "     czechoslovakia       0.00      0.00      0.00         1\n",
            "            denmark       0.00      0.00      0.00        19\n",
            "           djibouti       0.00      0.00      0.00         0\n",
            " dominican-republic       0.00      0.00      0.00         2\n",
            "       east-germany       0.00      0.00      0.00         1\n",
            "            ecuador       0.00      0.00      0.00        27\n",
            "              egypt       0.00      0.00      0.00        12\n",
            "        el-salvador       0.00      0.00      0.00         3\n",
            "           ethiopia       0.00      0.00      0.00         1\n",
            "               fiji       0.00      0.00      0.00         1\n",
            "            finland       0.00      0.00      0.00         9\n",
            "             france       0.00      0.00      0.00       155\n",
            "              gabon       0.00      0.00      0.00         1\n",
            "              ghana       0.00      0.00      0.00         6\n",
            "             greece       0.00      0.00      0.00        11\n",
            "               guam       0.00      0.00      0.00         0\n",
            "          guatemala       0.00      0.00      0.00         5\n",
            "             guinea       0.00      0.00      0.00         1\n",
            "             guyana       0.00      0.00      0.00         2\n",
            "              haiti       0.00      0.00      0.00         4\n",
            "           honduras       0.00      0.00      0.00         4\n",
            "          hong-kong       0.00      0.00      0.00        43\n",
            "            hungary       0.00      0.00      0.00         7\n",
            "            iceland       0.00      0.00      0.00         1\n",
            "              india       0.00      0.00      0.00        29\n",
            "          indonesia       0.00      0.00      0.00        36\n",
            "               iran       0.00      0.00      0.00        58\n",
            "               iraq       0.00      0.00      0.00        29\n",
            "            ireland       0.00      0.00      0.00         3\n",
            "             israel       0.00      0.00      0.00         5\n",
            "              italy       0.00      0.00      0.00        56\n",
            "        ivory-coast       0.00      0.00      0.00         6\n",
            "            jamaica       0.00      0.00      0.00         2\n",
            "              japan       0.00      0.00      0.00       382\n",
            "             jordan       0.00      0.00      0.00         8\n",
            "          kampuchea       0.00      0.00      0.00         0\n",
            "              kenya       0.00      0.00      0.00         6\n",
            "             kuwait       0.00      0.00      0.00        17\n",
            "            lebanon       0.00      0.00      0.00         4\n",
            "            lesotho       0.00      0.00      0.00         0\n",
            "            liberia       0.00      0.00      0.00         1\n",
            "              libya       0.00      0.00      0.00         4\n",
            "      liechtenstein       0.00      0.00      0.00         0\n",
            "         luxembourg       0.00      0.00      0.00        17\n",
            "         madagascar       0.00      0.00      0.00         3\n",
            "             malawi       0.00      0.00      0.00         3\n",
            "           malaysia       0.00      0.00      0.00        32\n",
            "              malta       0.00      0.00      0.00         0\n",
            "         mauritania       0.00      0.00      0.00         0\n",
            "          mauritius       0.00      0.00      0.00         2\n",
            "             mexico       0.00      0.00      0.00        29\n",
            "            morocco       0.00      0.00      0.00         8\n",
            "         mozambique       0.00      0.00      0.00         2\n",
            "            namibia       0.00      0.00      0.00         0\n",
            "              nepal       0.00      0.00      0.00         2\n",
            "        netherlands       0.00      0.00      0.00        68\n",
            "        new-zealand       0.00      0.00      0.00        43\n",
            "          nicaragua       0.00      0.00      0.00         9\n",
            "              niger       0.00      0.00      0.00         0\n",
            "            nigeria       0.00      0.00      0.00        14\n",
            "        north-korea       0.00      0.00      0.00         4\n",
            "             norway       0.00      0.00      0.00        12\n",
            "               oman       0.00      0.00      0.00         0\n",
            "           pakistan       0.00      0.00      0.00        14\n",
            "             panama       0.00      0.00      0.00         4\n",
            "   papua-new-guinea       0.00      0.00      0.00         3\n",
            "           paraguay       0.00      0.00      0.00         1\n",
            "               peru       0.00      0.00      0.00        16\n",
            "        philippines       0.00      0.00      0.00        35\n",
            "             poland       0.00      0.00      0.00         6\n",
            "           portugal       0.00      0.00      0.00        12\n",
            "              qatar       0.00      0.00      0.00         2\n",
            "            romania       0.00      0.00      0.00         2\n",
            "             rwanda       0.00      0.00      0.00         0\n",
            "       saudi-arabia       0.00      0.00      0.00        27\n",
            "            senegal       0.00      0.00      0.00         2\n",
            "       sierra-leone       0.00      0.00      0.00         1\n",
            "          singapore       0.00      0.00      0.00        20\n",
            "            somalia       0.00      0.00      0.00         2\n",
            "       south-africa       0.00      0.00      0.00        37\n",
            "        south-korea       0.00      0.00      0.00        34\n",
            "              spain       0.00      0.00      0.00        40\n",
            "          sri-lanka       0.00      0.00      0.00        12\n",
            "              sudan       0.00      0.00      0.00         4\n",
            "           suriname       0.00      0.00      0.00         0\n",
            "          swaziland       0.00      0.00      0.00         0\n",
            "             sweden       0.00      0.00      0.00        34\n",
            "        switzerland       0.00      0.00      0.00        72\n",
            "              syria       0.00      0.00      0.00         4\n",
            "             taiwan       0.00      0.00      0.00        43\n",
            "           tanzania       0.00      0.00      0.00         6\n",
            "           thailand       0.00      0.00      0.00        32\n",
            "               togo       0.00      0.00      0.00         1\n",
            "    trinidad-tobago       0.00      0.00      0.00         0\n",
            "            tunisia       0.00      0.00      0.00         5\n",
            "             turkey       0.00      0.00      0.00        22\n",
            "                uae       0.00      0.00      0.00        14\n",
            "             uganda       0.00      0.00      0.00         6\n",
            "                 uk       1.00      0.00      0.00       468\n",
            "            uruguay       0.00      0.00      0.00         1\n",
            "  us-virgin-islands       0.00      0.00      0.00         1\n",
            "                usa       0.64      0.99      0.78      4145\n",
            "               ussr       0.00      0.00      0.00        59\n",
            "            vanuatu       0.00      0.00      0.00         1\n",
            "          venezuela       0.00      0.00      0.00        21\n",
            "            vietnam       0.00      0.00      0.00         3\n",
            "       west-germany       0.00      0.00      0.00       181\n",
            "yemen-arab-republic       0.00      0.00      0.00         3\n",
            "yemen-demo-republic       0.00      0.00      0.00         2\n",
            "         yugoslavia       0.00      0.00      0.00        14\n",
            "              zaire       0.00      0.00      0.00         4\n",
            "             zambia       0.00      0.00      0.00        10\n",
            "           zimbabwe       0.00      0.00      0.00         5\n",
            "\n",
            "          micro avg       0.64      0.49      0.56      8364\n",
            "          macro avg       0.02      0.01      0.01      8364\n",
            "       weighted avg       0.48      0.49      0.39      8364\n",
            "        samples avg       0.58      0.54      0.55      8364\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Usireut multinomial naive bayes yieled an accuracy of ---> 0.5114450217666058\n",
            "Usireut multinomial naive bayes yieled a micro f1-score of ---> 0.5565393714092599\n",
            "Usireut multinomial naive bayes yieled a macro f1-score of ---> 0.005417705217819377\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVJG0McMYikj"
      },
      "source": [
        "### Random Forest Classifier - Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6151nIHZOs_",
        "outputId": "7de8336c-3697-49a7-fb1b-cc05d6b04bfb"
      },
      "source": [
        "print('Available parameters to use for grid search----> ', rfc_reut_model.get_params().keys())\n",
        "parameters_rfc_reut = {\n",
        "    'bootstrap' : (True, False),\n",
        "    'n_estimators' : (100, 200, 500),\n",
        "    'max_depth' : (25, 50, 100)\n",
        "}"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Available parameters to use for grid search---->  dict_keys(['bootstrap', 'ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9S4BuVXZUHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "807dd06c-4e8f-44e5-f018-3b8d08d572af"
      },
      "source": [
        "grid_search_rfc_reut = GridSearchCV(rfc_reut_model, parameters_rfc_reut, cv=2, n_jobs=1, verbose=3)\n",
        "start = timer()\n",
        "grid_search_rfc_reut = grid_search_rfc_reut.fit(reut_x_train, reut_y_train)\n",
        "end = timer()\n",
        "print(\"Time taken to execute Grid search for Random Forest Classifier!\", round(end-start)) "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 18 candidates, totalling 36 fits\n",
            "[CV] bootstrap=True, max_depth=25, n_estimators=100 ..................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  bootstrap=True, max_depth=25, n_estimators=100, score=0.582, total=  35.1s\n",
            "[CV] bootstrap=True, max_depth=25, n_estimators=100 ..................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   35.1s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  bootstrap=True, max_depth=25, n_estimators=100, score=0.592, total=  35.1s\n",
            "[CV] bootstrap=True, max_depth=25, n_estimators=200 ..................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.2min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  bootstrap=True, max_depth=25, n_estimators=200, score=0.579, total= 1.2min\n",
            "[CV] bootstrap=True, max_depth=25, n_estimators=200 ..................\n",
            "[CV]  bootstrap=True, max_depth=25, n_estimators=200, score=0.590, total= 1.1min\n",
            "[CV] bootstrap=True, max_depth=25, n_estimators=500 ..................\n",
            "[CV]  bootstrap=True, max_depth=25, n_estimators=500, score=0.581, total= 2.7min\n",
            "[CV] bootstrap=True, max_depth=25, n_estimators=500 ..................\n",
            "[CV]  bootstrap=True, max_depth=25, n_estimators=500, score=0.590, total= 2.8min\n",
            "[CV] bootstrap=True, max_depth=50, n_estimators=100 ..................\n",
            "[CV]  bootstrap=True, max_depth=50, n_estimators=100, score=0.601, total=  48.9s\n",
            "[CV] bootstrap=True, max_depth=50, n_estimators=100 ..................\n",
            "[CV]  bootstrap=True, max_depth=50, n_estimators=100, score=0.614, total=  48.5s\n",
            "[CV] bootstrap=True, max_depth=50, n_estimators=200 ..................\n",
            "[CV]  bootstrap=True, max_depth=50, n_estimators=200, score=0.603, total= 1.6min\n",
            "[CV] bootstrap=True, max_depth=50, n_estimators=200 ..................\n",
            "[CV]  bootstrap=True, max_depth=50, n_estimators=200, score=0.613, total= 1.6min\n",
            "[CV] bootstrap=True, max_depth=50, n_estimators=500 ..................\n",
            "[CV]  bootstrap=True, max_depth=50, n_estimators=500, score=0.603, total= 3.8min\n",
            "[CV] bootstrap=True, max_depth=50, n_estimators=500 ..................\n",
            "[CV]  bootstrap=True, max_depth=50, n_estimators=500, score=0.614, total= 3.8min\n",
            "[CV] bootstrap=True, max_depth=100, n_estimators=100 .................\n",
            "[CV]  bootstrap=True, max_depth=100, n_estimators=100, score=0.613, total=  56.2s\n",
            "[CV] bootstrap=True, max_depth=100, n_estimators=100 .................\n",
            "[CV]  bootstrap=True, max_depth=100, n_estimators=100, score=0.621, total=  57.2s\n",
            "[CV] bootstrap=True, max_depth=100, n_estimators=200 .................\n",
            "[CV]  bootstrap=True, max_depth=100, n_estimators=200, score=0.612, total= 1.9min\n",
            "[CV] bootstrap=True, max_depth=100, n_estimators=200 .................\n",
            "[CV]  bootstrap=True, max_depth=100, n_estimators=200, score=0.623, total= 1.9min\n",
            "[CV] bootstrap=True, max_depth=100, n_estimators=500 .................\n",
            "[CV]  bootstrap=True, max_depth=100, n_estimators=500, score=0.613, total= 4.8min\n",
            "[CV] bootstrap=True, max_depth=100, n_estimators=500 .................\n",
            "[CV]  bootstrap=True, max_depth=100, n_estimators=500, score=0.623, total= 4.7min\n",
            "[CV] bootstrap=False, max_depth=25, n_estimators=100 .................\n",
            "[CV]  bootstrap=False, max_depth=25, n_estimators=100, score=0.584, total=  40.6s\n",
            "[CV] bootstrap=False, max_depth=25, n_estimators=100 .................\n",
            "[CV]  bootstrap=False, max_depth=25, n_estimators=100, score=0.590, total=  39.9s\n",
            "[CV] bootstrap=False, max_depth=25, n_estimators=200 .................\n",
            "[CV]  bootstrap=False, max_depth=25, n_estimators=200, score=0.585, total= 1.4min\n",
            "[CV] bootstrap=False, max_depth=25, n_estimators=200 .................\n",
            "[CV]  bootstrap=False, max_depth=25, n_estimators=200, score=0.593, total= 1.4min\n",
            "[CV] bootstrap=False, max_depth=25, n_estimators=500 .................\n",
            "[CV]  bootstrap=False, max_depth=25, n_estimators=500, score=0.584, total= 3.5min\n",
            "[CV] bootstrap=False, max_depth=25, n_estimators=500 .................\n",
            "[CV]  bootstrap=False, max_depth=25, n_estimators=500, score=0.589, total= 3.5min\n",
            "[CV] bootstrap=False, max_depth=50, n_estimators=100 .................\n",
            "[CV]  bootstrap=False, max_depth=50, n_estimators=100, score=0.610, total= 1.1min\n",
            "[CV] bootstrap=False, max_depth=50, n_estimators=100 .................\n",
            "[CV]  bootstrap=False, max_depth=50, n_estimators=100, score=0.621, total= 1.1min\n",
            "[CV] bootstrap=False, max_depth=50, n_estimators=200 .................\n",
            "[CV]  bootstrap=False, max_depth=50, n_estimators=200, score=0.609, total= 2.1min\n",
            "[CV] bootstrap=False, max_depth=50, n_estimators=200 .................\n",
            "[CV]  bootstrap=False, max_depth=50, n_estimators=200, score=0.622, total= 2.1min\n",
            "[CV] bootstrap=False, max_depth=50, n_estimators=500 .................\n",
            "[CV]  bootstrap=False, max_depth=50, n_estimators=500, score=0.609, total= 5.4min\n",
            "[CV] bootstrap=False, max_depth=50, n_estimators=500 .................\n",
            "[CV]  bootstrap=False, max_depth=50, n_estimators=500, score=0.621, total= 5.3min\n",
            "[CV] bootstrap=False, max_depth=100, n_estimators=100 ................\n",
            "[CV]  bootstrap=False, max_depth=100, n_estimators=100, score=0.619, total= 1.3min\n",
            "[CV] bootstrap=False, max_depth=100, n_estimators=100 ................\n",
            "[CV]  bootstrap=False, max_depth=100, n_estimators=100, score=0.629, total= 1.3min\n",
            "[CV] bootstrap=False, max_depth=100, n_estimators=200 ................\n",
            "[CV]  bootstrap=False, max_depth=100, n_estimators=200, score=0.620, total= 2.7min\n",
            "[CV] bootstrap=False, max_depth=100, n_estimators=200 ................\n",
            "[CV]  bootstrap=False, max_depth=100, n_estimators=200, score=0.630, total= 2.7min\n",
            "[CV] bootstrap=False, max_depth=100, n_estimators=500 ................\n",
            "[CV]  bootstrap=False, max_depth=100, n_estimators=500, score=0.619, total= 6.7min\n",
            "[CV] bootstrap=False, max_depth=100, n_estimators=500 ................\n",
            "[CV]  bootstrap=False, max_depth=100, n_estimators=500, score=0.629, total= 6.9min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed: 86.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time taken to execute Grid search for Random Forest Classifier! 5528\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5IrJcFcZUNP",
        "outputId": "5dca4130-d5d8-4dd2-cf34-2a9f542adbd6"
      },
      "source": [
        "grid_search_rfc_reut_y_pred = grid_search_rfc_reut.predict(reut_x_test)\n",
        "print('SCORES WITH RANDOM FOREST CLASSIFIER AFTER GRID SEARCH')\n",
        "print('\\n\\n')\n",
        "print(classification_report(reut_y_test, grid_search_rfc_reut_y_pred, zero_division='warn'))\n",
        "print('\\n\\n')\n",
        "print(\"Using random forest classifier yieled an accuracy of --->\", grid_search_rfc_reut.score(reut_x_test, reut_y_test))\n",
        "print(\"Using random forest classifier0 a micro f1-score of --->\", f1_score(reut_y_test, grid_search_rfc_reut_y_pred, average='micro'))\n",
        "print(\"Using random forest classifier a macro f1-score of --->\", f1_score(reut_y_test, grid_search_rfc_reut_y_pred, average='macro'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SCORES WITH RANDOM FOREST CLASSIFIER AFTER GRID SEARCH\n",
            "\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.81      0.84       884\n",
            "           1       0.00      0.00      0.00         1\n",
            "           2       0.00      0.00      0.00        11\n",
            "           3       0.00      0.00      0.00         1\n",
            "           4       0.00      0.00      0.00         0\n",
            "           5       1.00      0.07      0.13        28\n",
            "           6       0.00      0.00      0.00         1\n",
            "           7       1.00      0.08      0.15        85\n",
            "           8       0.00      0.00      0.00         6\n",
            "           9       0.00      0.00      0.00         0\n",
            "          10       0.00      0.00      0.00        13\n",
            "          11       1.00      0.09      0.17        11\n",
            "          12       0.00      0.00      0.00         0\n",
            "          13       0.83      0.07      0.13        69\n",
            "          14       0.00      0.00      0.00         0\n",
            "          15       0.00      0.00      0.00         2\n",
            "          16       0.00      0.00      0.00         0\n",
            "          17       1.00      0.12      0.22         8\n",
            "          18       0.00      0.00      0.00         1\n",
            "          19       1.00      0.10      0.19       106\n",
            "          20       0.00      0.00      0.00         1\n",
            "          21       0.00      0.00      0.00         0\n",
            "          22       0.00      0.00      0.00         0\n",
            "          23       0.00      0.00      0.00         1\n",
            "          24       0.00      0.00      0.00         1\n",
            "          25       0.95      0.09      0.17       385\n",
            "          26       0.00      0.00      0.00         0\n",
            "          27       0.00      0.00      0.00         1\n",
            "          28       0.00      0.00      0.00         4\n",
            "          29       1.00      0.02      0.03        65\n",
            "          30       1.00      0.17      0.29        12\n",
            "          31       0.00      0.00      0.00         2\n",
            "          32       0.00      0.00      0.00         8\n",
            "          33       1.00      0.11      0.20         9\n",
            "          34       0.00      0.00      0.00         7\n",
            "          35       0.00      0.00      0.00         1\n",
            "          36       1.00      0.21      0.35        19\n",
            "          37       0.00      0.00      0.00         0\n",
            "          38       0.00      0.00      0.00         2\n",
            "          39       0.00      0.00      0.00         1\n",
            "          40       0.00      0.00      0.00        27\n",
            "          41       1.00      0.08      0.15        12\n",
            "          42       0.00      0.00      0.00         3\n",
            "          43       0.00      0.00      0.00         1\n",
            "          44       0.00      0.00      0.00         1\n",
            "          45       1.00      0.11      0.20         9\n",
            "          46       0.71      0.03      0.06       155\n",
            "          47       0.00      0.00      0.00         1\n",
            "          48       1.00      0.33      0.50         6\n",
            "          49       1.00      0.18      0.31        11\n",
            "          50       0.00      0.00      0.00         0\n",
            "          51       0.00      0.00      0.00         5\n",
            "          52       0.00      0.00      0.00         1\n",
            "          53       0.00      0.00      0.00         2\n",
            "          54       0.00      0.00      0.00         4\n",
            "          55       0.00      0.00      0.00         4\n",
            "          56       0.80      0.09      0.17        43\n",
            "          57       1.00      0.14      0.25         7\n",
            "          58       0.00      0.00      0.00         1\n",
            "          59       1.00      0.07      0.13        29\n",
            "          60       1.00      0.11      0.20        36\n",
            "          61       1.00      0.17      0.29        58\n",
            "          62       1.00      0.17      0.29        29\n",
            "          63       1.00      0.33      0.50         3\n",
            "          64       0.00      0.00      0.00         5\n",
            "          65       0.83      0.09      0.16        56\n",
            "          66       1.00      0.17      0.29         6\n",
            "          67       0.00      0.00      0.00         2\n",
            "          68       0.99      0.30      0.46       382\n",
            "          69       0.00      0.00      0.00         8\n",
            "          70       0.00      0.00      0.00         0\n",
            "          71       0.00      0.00      0.00         6\n",
            "          72       1.00      0.12      0.21        17\n",
            "          73       0.00      0.00      0.00         4\n",
            "          74       0.00      0.00      0.00         0\n",
            "          75       0.00      0.00      0.00         1\n",
            "          76       0.00      0.00      0.00         4\n",
            "          77       0.00      0.00      0.00         0\n",
            "          78       0.00      0.00      0.00        17\n",
            "          79       0.00      0.00      0.00         3\n",
            "          80       0.00      0.00      0.00         3\n",
            "          81       0.00      0.00      0.00        32\n",
            "          82       0.00      0.00      0.00         0\n",
            "          83       0.00      0.00      0.00         0\n",
            "          84       0.00      0.00      0.00         2\n",
            "          85       1.00      0.03      0.07        29\n",
            "          86       1.00      0.12      0.22         8\n",
            "          87       0.00      0.00      0.00         2\n",
            "          88       0.00      0.00      0.00         0\n",
            "          89       0.00      0.00      0.00         2\n",
            "          90       1.00      0.06      0.11        68\n",
            "          91       0.67      0.05      0.09        43\n",
            "          92       0.00      0.00      0.00         9\n",
            "          93       0.00      0.00      0.00         0\n",
            "          94       0.00      0.00      0.00        14\n",
            "          95       0.00      0.00      0.00         4\n",
            "          96       0.00      0.00      0.00        12\n",
            "          97       0.00      0.00      0.00         0\n",
            "          98       0.00      0.00      0.00        14\n",
            "          99       0.00      0.00      0.00         4\n",
            "         100       0.00      0.00      0.00         3\n",
            "         101       0.00      0.00      0.00         1\n",
            "         102       0.00      0.00      0.00        16\n",
            "         103       0.88      0.20      0.33        35\n",
            "         104       0.00      0.00      0.00         6\n",
            "         105       0.00      0.00      0.00        12\n",
            "         106       0.00      0.00      0.00         2\n",
            "         107       0.00      0.00      0.00         2\n",
            "         108       0.00      0.00      0.00         0\n",
            "         109       0.75      0.11      0.19        27\n",
            "         110       0.00      0.00      0.00         2\n",
            "         111       0.00      0.00      0.00         1\n",
            "         112       0.00      0.00      0.00        20\n",
            "         113       0.00      0.00      0.00         2\n",
            "         114       1.00      0.14      0.24        37\n",
            "         115       0.00      0.00      0.00        34\n",
            "         116       1.00      0.05      0.10        40\n",
            "         117       0.00      0.00      0.00        12\n",
            "         118       0.00      0.00      0.00         4\n",
            "         119       0.00      0.00      0.00         0\n",
            "         120       0.00      0.00      0.00         0\n",
            "         121       1.00      0.15      0.26        34\n",
            "         122       1.00      0.11      0.20        72\n",
            "         123       0.00      0.00      0.00         4\n",
            "         124       1.00      0.19      0.31        43\n",
            "         125       1.00      0.17      0.29         6\n",
            "         126       0.00      0.00      0.00        32\n",
            "         127       0.00      0.00      0.00         1\n",
            "         128       0.00      0.00      0.00         0\n",
            "         129       0.00      0.00      0.00         5\n",
            "         130       1.00      0.09      0.17        22\n",
            "         131       1.00      0.07      0.13        14\n",
            "         132       1.00      0.17      0.29         6\n",
            "         133       0.91      0.29      0.44       468\n",
            "         134       0.00      0.00      0.00         1\n",
            "         135       0.00      0.00      0.00         1\n",
            "         136       0.87      0.96      0.91      4145\n",
            "         137       1.00      0.07      0.13        59\n",
            "         138       0.00      0.00      0.00         1\n",
            "         139       1.00      0.05      0.09        21\n",
            "         140       0.00      0.00      0.00         3\n",
            "         141       0.95      0.10      0.18       181\n",
            "         142       0.00      0.00      0.00         3\n",
            "         143       0.00      0.00      0.00         2\n",
            "         144       1.00      0.14      0.25        14\n",
            "         145       0.00      0.00      0.00         4\n",
            "         146       1.00      0.10      0.18        10\n",
            "         147       0.00      0.00      0.00         5\n",
            "\n",
            "   micro avg       0.88      0.61      0.72      8364\n",
            "   macro avg       0.32      0.05      0.08      8364\n",
            "weighted avg       0.85      0.61      0.63      8364\n",
            " samples avg       0.70      0.68      0.68      8364\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using multinomial naive bayes yieled an accuracy of ---> 0.6476618452464542\n",
            "Using multinomial naive bayes a micro f1-score of ---> 0.7222612853325833\n",
            "Using multinomial naive bayes a macro f1-score of ---> 0.08251650316175263\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swMeIXaQJ_yP"
      },
      "source": [
        "## Non-Probabilistic Model - Linear Support Vector Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJsOY_WQIbgA"
      },
      "source": [
        "lsvc_reut_model = LinearSVC()\n",
        "\n",
        "tf_vectorizer = TfidfVectorizer() # stop_words='english', ngram_range=(1, 2), norm='l2')\n",
        "X = tf_vectorizer.fit_transform(X_raw)\n",
        "\n",
        "le = LabelEncoder()\n",
        "string_labels = [', '.join(ele) for ele in labels]\n",
        "y = le.fit_transform(string_labels)\n",
        "\n",
        "\n",
        "reut_x_train, reut_x_test, reut_y_train, reut_y_test = train_test_split(X, y, \n",
        "                                                        test_size=0.33, random_state=0)\n",
        "\n",
        "# https://github.com/davidsbatista/text-classification/issues/1"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tnfwFwVKS7P",
        "outputId": "9ef7f61d-e062-4295-fdad-3d6b1dd2ae0d"
      },
      "source": [
        "print(\"Linear Support Vector Classifier\")\n",
        "print()\n",
        "start = timer()\n",
        "lsvc_reut_model.fit(reut_x_train, reut_y_train)\n",
        "end = timer()\n",
        "print()\n",
        "print('Time taken in seconds to fit the model!', round(end-start, 5))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear Support Vector Classifier\n",
            "\n",
            "\n",
            "Time taken in seconds to fit the model! 43.57174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmuDA7-2NSu4",
        "outputId": "d013c4b4-576b-4f2c-9561-75abfe9ea35a"
      },
      "source": [
        "lsvc_reut_y_pred = lsvc_reut_model.predict(reut_x_test)\n",
        "print(classification_report(reut_y_test, lsvc_reut_y_pred,zero_division='warn'))\n",
        "print('\\n\\n')\n",
        "print(\"Using Linear Support Vector Classification yieled an accuracy of --->\", lsvc_reut_model.score(reut_x_test, reut_y_test))\n",
        "print(\"Using support vector classification yielded a micro f1-score of --->\", f1_score(reut_y_test, lsvc_reut_y_pred, average='micro'))\n",
        "print(\"Using support vector classification yielded a micro f1-score of --->\", f1_score(reut_y_test, lsvc_reut_y_pred, average='macro'))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.87      0.85       884\n",
            "           3       0.00      0.00      0.00         1\n",
            "           4       0.00      0.00      0.00         2\n",
            "           6       0.00      0.00      0.00         0\n",
            "           7       0.88      0.58      0.70        12\n",
            "           8       0.00      0.00      0.00         2\n",
            "           9       0.00      0.00      0.00         0\n",
            "          10       0.00      0.00      0.00         1\n",
            "          11       0.00      0.00      0.00         0\n",
            "          12       0.00      0.00      0.00         1\n",
            "          13       1.00      1.00      1.00         1\n",
            "          14       0.00      0.00      0.00         1\n",
            "          15       0.87      0.87      0.87        60\n",
            "          16       0.00      0.00      0.00         1\n",
            "          19       0.00      0.00      0.00         0\n",
            "          20       0.00      0.00      0.00         0\n",
            "          21       0.00      0.00      0.00         1\n",
            "          22       0.00      0.00      0.00         1\n",
            "          24       0.00      0.00      0.00         1\n",
            "          25       0.00      0.00      0.00         1\n",
            "          26       1.00      0.50      0.67         4\n",
            "          28       0.00      0.00      0.00         1\n",
            "          30       0.75      0.75      0.75         4\n",
            "          31       0.00      0.00      0.00         0\n",
            "          33       0.00      0.00      0.00         1\n",
            "          36       0.00      0.00      0.00         1\n",
            "          37       0.00      0.00      0.00         2\n",
            "          41       0.89      0.89      0.89         9\n",
            "          42       0.00      0.00      0.00         1\n",
            "          46       0.53      0.67      0.59        43\n",
            "          48       0.00      0.00      0.00         1\n",
            "          50       0.00      0.00      0.00         2\n",
            "          55       0.00      0.00      0.00         1\n",
            "          57       0.00      0.00      0.00         0\n",
            "          60       0.00      0.00      0.00         1\n",
            "          61       0.00      0.00      0.00         1\n",
            "          63       0.00      0.00      0.00         1\n",
            "          64       0.00      0.00      0.00         1\n",
            "          66       0.00      0.00      0.00         1\n",
            "          67       0.00      0.00      0.00         1\n",
            "          69       0.00      0.00      0.00         1\n",
            "          71       0.00      0.00      0.00         1\n",
            "          74       1.00      0.50      0.67         2\n",
            "          78       0.00      0.00      0.00         1\n",
            "          79       0.00      0.00      0.00         1\n",
            "          83       0.00      0.00      0.00         1\n",
            "          84       0.00      0.00      0.00         1\n",
            "          85       0.00      0.00      0.00         2\n",
            "          86       1.00      0.60      0.75         5\n",
            "          89       0.00      0.00      0.00         1\n",
            "          91       0.71      0.82      0.76        45\n",
            "          93       0.00      0.00      0.00         1\n",
            "          97       0.00      0.00      0.00         1\n",
            "          98       0.00      0.00      0.00         1\n",
            "         100       0.00      0.00      0.00         1\n",
            "         103       0.00      0.00      0.00         3\n",
            "         106       0.00      0.00      0.00         4\n",
            "         107       0.00      0.00      0.00         1\n",
            "         110       1.00      1.00      1.00         1\n",
            "         111       0.00      0.00      0.00         0\n",
            "         112       0.00      0.00      0.00         1\n",
            "         113       0.77      0.54      0.64       289\n",
            "         114       0.00      0.00      0.00         1\n",
            "         116       0.00      0.00      0.00         1\n",
            "         118       0.00      0.00      0.00         1\n",
            "         119       0.00      0.00      0.00         1\n",
            "         122       0.40      0.33      0.36         6\n",
            "         124       0.00      0.00      0.00         1\n",
            "         127       0.00      0.00      0.00         1\n",
            "         130       0.00      0.00      0.00         2\n",
            "         131       0.00      0.00      0.00        12\n",
            "         133       0.00      0.00      0.00         1\n",
            "         135       0.00      0.00      0.00         1\n",
            "         137       0.00      0.00      0.00         2\n",
            "         138       0.66      0.89      0.76        28\n",
            "         139       0.00      0.00      0.00         1\n",
            "         142       0.00      0.00      0.00         1\n",
            "         143       0.00      0.00      0.00         1\n",
            "         145       0.00      0.00      0.00         1\n",
            "         146       0.00      0.00      0.00         1\n",
            "         147       0.50      1.00      0.67         1\n",
            "         149       0.00      0.00      0.00         1\n",
            "         150       0.00      0.00      0.00         1\n",
            "         152       0.00      0.00      0.00         2\n",
            "         153       0.00      0.00      0.00         1\n",
            "         156       0.80      0.80      0.80         5\n",
            "         157       0.00      0.00      0.00         1\n",
            "         161       0.00      0.00      0.00         3\n",
            "         162       0.00      0.00      0.00         1\n",
            "         164       0.00      0.00      0.00         5\n",
            "         165       1.00      1.00      1.00         1\n",
            "         167       0.00      0.00      0.00         2\n",
            "         169       0.00      0.00      0.00         1\n",
            "         171       0.00      0.00      0.00         1\n",
            "         173       0.00      0.00      0.00         0\n",
            "         174       0.00      0.00      0.00         1\n",
            "         176       0.83      0.62      0.71         8\n",
            "         177       0.00      0.00      0.00         1\n",
            "         178       0.00      0.00      0.00         1\n",
            "         179       0.00      0.00      0.00         1\n",
            "         180       0.00      0.00      0.00         1\n",
            "         182       0.72      1.00      0.84        13\n",
            "         186       0.00      0.00      0.00         1\n",
            "         187       1.00      1.00      1.00         1\n",
            "         188       0.00      0.00      0.00         1\n",
            "         189       0.00      0.00      0.00         1\n",
            "         191       0.00      0.00      0.00         1\n",
            "         193       1.00      1.00      1.00         1\n",
            "         196       0.00      0.00      0.00         0\n",
            "         198       0.00      0.00      0.00         1\n",
            "         199       0.75      0.86      0.80         7\n",
            "         200       0.00      0.00      0.00         1\n",
            "         203       0.00      0.00      0.00         1\n",
            "         206       0.69      0.76      0.72        97\n",
            "         207       0.00      0.00      0.00         1\n",
            "         208       0.00      0.00      0.00         1\n",
            "         210       0.00      0.00      0.00         1\n",
            "         211       0.00      0.00      0.00         1\n",
            "         212       0.00      0.00      0.00         0\n",
            "         218       0.00      0.00      0.00         0\n",
            "         220       0.00      0.00      0.00         1\n",
            "         221       0.00      0.00      0.00         1\n",
            "         229       1.00      1.00      1.00         1\n",
            "         230       0.00      0.00      0.00         1\n",
            "         232       0.00      0.00      0.00         1\n",
            "         233       0.00      0.00      0.00         1\n",
            "         236       0.00      0.00      0.00         1\n",
            "         237       1.00      1.00      1.00         1\n",
            "         242       0.00      0.00      0.00         3\n",
            "         245       0.00      0.00      0.00         1\n",
            "         247       0.00      0.00      0.00         0\n",
            "         248       1.00      1.00      1.00         2\n",
            "         250       0.00      0.00      0.00         2\n",
            "         255       0.00      0.00      0.00         2\n",
            "         256       0.00      0.00      0.00         1\n",
            "         257       1.00      1.00      1.00         1\n",
            "         259       0.67      1.00      0.80         2\n",
            "         260       0.00      0.00      0.00         1\n",
            "         261       0.00      0.00      0.00         1\n",
            "         262       0.00      0.00      0.00         2\n",
            "         263       0.00      0.00      0.00         1\n",
            "         264       0.00      0.00      0.00         0\n",
            "         265       0.71      0.71      0.71        24\n",
            "         267       0.00      0.00      0.00         1\n",
            "         270       0.00      0.00      0.00         2\n",
            "         271       0.00      0.00      0.00         1\n",
            "         273       0.00      0.00      0.00         1\n",
            "         274       0.00      0.00      0.00         2\n",
            "         277       0.00      0.00      0.00         1\n",
            "         279       0.00      0.00      0.00         1\n",
            "         281       0.00      0.00      0.00         1\n",
            "         283       0.00      0.00      0.00         1\n",
            "         284       1.00      1.00      1.00         1\n",
            "         287       1.00      0.75      0.86         4\n",
            "         288       0.00      0.00      0.00         1\n",
            "         289       0.78      0.58      0.67        12\n",
            "         292       1.00      1.00      1.00         1\n",
            "         294       0.00      0.00      0.00         1\n",
            "         295       0.00      0.00      0.00         0\n",
            "         297       0.70      0.95      0.81        20\n",
            "         300       0.00      0.00      0.00         1\n",
            "         301       0.00      0.00      0.00         1\n",
            "         302       0.00      0.00      0.00         1\n",
            "         304       0.00      0.00      0.00         1\n",
            "         305       0.00      0.00      0.00         1\n",
            "         307       0.00      0.00      0.00         2\n",
            "         310       0.00      0.00      0.00         0\n",
            "         313       0.71      0.45      0.56        11\n",
            "         314       0.00      0.00      0.00         1\n",
            "         315       0.00      0.00      0.00         1\n",
            "         316       0.00      0.00      0.00         2\n",
            "         319       0.00      0.00      0.00         1\n",
            "         320       0.00      0.00      0.00         0\n",
            "         322       0.00      0.00      0.00         1\n",
            "         323       0.00      0.00      0.00         1\n",
            "         326       0.00      0.00      0.00         0\n",
            "         327       0.00      0.00      0.00         1\n",
            "         329       0.00      0.00      0.00         0\n",
            "         331       0.82      0.75      0.78        36\n",
            "         336       0.00      0.00      0.00         1\n",
            "         337       0.00      0.00      0.00         1\n",
            "         340       0.00      0.00      0.00         1\n",
            "         341       0.00      0.00      0.00         2\n",
            "         342       0.00      0.00      0.00         1\n",
            "         343       0.00      0.00      0.00         1\n",
            "         344       0.00      0.00      0.00         0\n",
            "         346       0.60      0.75      0.67         4\n",
            "         348       0.00      0.00      0.00         0\n",
            "         349       0.79      0.78      0.79       190\n",
            "         351       1.00      0.50      0.67         2\n",
            "         354       0.00      0.00      0.00         1\n",
            "         357       0.33      0.33      0.33         3\n",
            "         363       0.00      0.00      0.00         0\n",
            "         365       0.00      0.00      0.00         1\n",
            "         367       0.00      0.00      0.00         1\n",
            "         371       0.00      0.00      0.00         0\n",
            "         372       0.00      0.00      0.00         1\n",
            "         374       0.00      0.00      0.00         1\n",
            "         379       1.00      1.00      1.00         1\n",
            "         381       0.00      0.00      0.00         1\n",
            "         383       0.00      0.00      0.00         1\n",
            "         386       0.00      0.00      0.00         1\n",
            "         387       0.50      1.00      0.67         1\n",
            "         389       0.00      0.00      0.00         2\n",
            "         392       0.00      0.00      0.00         1\n",
            "         393       0.00      0.00      0.00         1\n",
            "         394       0.00      0.00      0.00         3\n",
            "         395       0.33      1.00      0.50         1\n",
            "         397       0.00      0.00      0.00         1\n",
            "         399       0.09      0.11      0.10        28\n",
            "         400       0.00      0.00      0.00         1\n",
            "         401       0.00      0.00      0.00         0\n",
            "         404       0.00      0.00      0.00         1\n",
            "         405       0.00      0.00      0.00         0\n",
            "         407       0.00      0.00      0.00         1\n",
            "         412       1.00      1.00      1.00         1\n",
            "         414       0.00      0.00      0.00         1\n",
            "         416       1.00      1.00      1.00         2\n",
            "         417       0.00      0.00      0.00         1\n",
            "         418       0.67      0.67      0.67         3\n",
            "         419       0.00      0.00      0.00         1\n",
            "         422       0.67      0.50      0.57         4\n",
            "         423       0.00      0.00      0.00         1\n",
            "         425       0.00      0.00      0.00         1\n",
            "         428       0.00      0.00      0.00         1\n",
            "         431       0.00      0.00      0.00         1\n",
            "         437       0.40      0.33      0.36         6\n",
            "         438       0.00      0.00      0.00         1\n",
            "         440       0.00      0.00      0.00         1\n",
            "         442       0.00      0.00      0.00         1\n",
            "         445       0.00      0.00      0.00         1\n",
            "         446       0.00      0.00      0.00         1\n",
            "         449       0.00      0.00      0.00         1\n",
            "         450       0.00      0.00      0.00         2\n",
            "         451       1.00      0.50      0.67         2\n",
            "         452       0.00      0.00      0.00         1\n",
            "         453       0.79      1.00      0.88        15\n",
            "         454       0.00      0.00      0.00         1\n",
            "         456       0.00      0.00      0.00         1\n",
            "         458       0.00      0.00      0.00         1\n",
            "         460       0.00      0.00      0.00         0\n",
            "         461       0.00      0.00      0.00         2\n",
            "         462       0.00      0.00      0.00         0\n",
            "         463       0.00      0.00      0.00         1\n",
            "         465       0.00      0.00      0.00         1\n",
            "         466       1.00      1.00      1.00         1\n",
            "         467       0.00      0.00      0.00         0\n",
            "         468       0.00      0.00      0.00         2\n",
            "         469       0.00      0.00      0.00         1\n",
            "         470       0.00      0.00      0.00         1\n",
            "         472       0.50      1.00      0.67         1\n",
            "         473       0.00      0.00      0.00         1\n",
            "         475       0.88      0.86      0.87        50\n",
            "         478       0.00      0.00      0.00         1\n",
            "         479       0.00      0.00      0.00         1\n",
            "         484       0.00      0.00      0.00         1\n",
            "         487       0.00      0.00      0.00         1\n",
            "         488       0.00      0.00      0.00         3\n",
            "         489       0.00      0.00      0.00         1\n",
            "         491       0.96      0.68      0.79        34\n",
            "         492       1.00      1.00      1.00         1\n",
            "         494       1.00      1.00      1.00         1\n",
            "         495       0.00      0.00      0.00         1\n",
            "         496       0.00      0.00      0.00         2\n",
            "         498       1.00      1.00      1.00         1\n",
            "         499       0.00      0.00      0.00         1\n",
            "         500       0.75      0.43      0.55         7\n",
            "         502       0.00      0.00      0.00         1\n",
            "         503       0.00      0.00      0.00         1\n",
            "         504       0.00      0.00      0.00         1\n",
            "         505       0.00      0.00      0.00         1\n",
            "         506       0.00      0.00      0.00         1\n",
            "         507       0.86      0.75      0.80         8\n",
            "         510       1.00      1.00      1.00         1\n",
            "         513       0.60      0.60      0.60         5\n",
            "         514       0.50      1.00      0.67         1\n",
            "         516       0.00      0.00      0.00         1\n",
            "         521       0.89      0.80      0.84        10\n",
            "         522       0.50      1.00      0.67         1\n",
            "         523       0.67      0.71      0.69        17\n",
            "         525       0.00      0.00      0.00         1\n",
            "         527       0.00      0.00      0.00         1\n",
            "         528       0.00      0.00      0.00         1\n",
            "         529       0.00      0.00      0.00         0\n",
            "         530       0.00      0.00      0.00         1\n",
            "         531       0.00      0.00      0.00         2\n",
            "         532       0.00      0.00      0.00         1\n",
            "         534       1.00      0.33      0.50         3\n",
            "         535       0.00      0.00      0.00         0\n",
            "         536       1.00      0.60      0.75         5\n",
            "         539       0.50      0.71      0.59         7\n",
            "         541       0.33      1.00      0.50         1\n",
            "         542       0.00      0.00      0.00         1\n",
            "         545       0.83      1.00      0.91        10\n",
            "         547       0.00      0.00      0.00         1\n",
            "         548       0.00      0.00      0.00         2\n",
            "         549       0.00      0.00      0.00         1\n",
            "         551       0.00      0.00      0.00         1\n",
            "         552       0.88      0.84      0.86        25\n",
            "         553       0.00      0.00      0.00         2\n",
            "         554       0.71      0.77      0.74        13\n",
            "         557       0.00      0.00      0.00         0\n",
            "         558       1.00      0.67      0.80         3\n",
            "         559       0.00      0.00      0.00         1\n",
            "         560       0.80      0.89      0.84        18\n",
            "         564       0.00      0.00      0.00         1\n",
            "         565       0.00      0.00      0.00         1\n",
            "         566       0.50      0.50      0.50         6\n",
            "         569       0.00      0.00      0.00         2\n",
            "         570       0.33      1.00      0.50         1\n",
            "         571       0.00      0.00      0.00         0\n",
            "         573       0.67      0.80      0.73        15\n",
            "         577       0.00      0.00      0.00         1\n",
            "         585       1.00      0.57      0.73         7\n",
            "         587       0.70      0.82      0.75        49\n",
            "         592       0.00      0.00      0.00         3\n",
            "         593       0.00      0.00      0.00         1\n",
            "         594       0.00      0.00      0.00         1\n",
            "         596       0.00      0.00      0.00         1\n",
            "         598       0.00      0.00      0.00         1\n",
            "         600       0.00      0.00      0.00         0\n",
            "         601       0.00      0.00      0.00         1\n",
            "         603       0.00      0.00      0.00         1\n",
            "         607       1.00      1.00      1.00         1\n",
            "         609       0.00      0.00      0.00         2\n",
            "         613       0.65      0.69      0.67        16\n",
            "         615       0.00      0.00      0.00         1\n",
            "         616       0.00      0.00      0.00         1\n",
            "         617       0.00      0.00      0.00         1\n",
            "         622       0.00      0.00      0.00         1\n",
            "         623       0.67      0.57      0.62         7\n",
            "         624       0.00      0.00      0.00         1\n",
            "         626       0.75      1.00      0.86         3\n",
            "         627       1.00      1.00      1.00         1\n",
            "         628       0.00      0.00      0.00         1\n",
            "         629       0.71      0.83      0.77        12\n",
            "         630       0.00      0.00      0.00         0\n",
            "         634       0.00      0.00      0.00         1\n",
            "         637       0.00      0.00      0.00         1\n",
            "         638       0.00      0.00      0.00         0\n",
            "         639       0.00      0.00      0.00         1\n",
            "         641       0.00      0.00      0.00         1\n",
            "         643       0.00      0.00      0.00         1\n",
            "         645       0.00      0.00      0.00         1\n",
            "         646       0.71      0.83      0.77         6\n",
            "         647       1.00      1.00      1.00         1\n",
            "         648       1.00      1.00      1.00         1\n",
            "         650       0.00      0.00      0.00         1\n",
            "         651       0.00      0.00      0.00         1\n",
            "         653       0.62      0.71      0.67         7\n",
            "         654       0.00      0.00      0.00         1\n",
            "         657       0.00      0.00      0.00         1\n",
            "         658       0.00      0.00      0.00         1\n",
            "         664       0.00      0.00      0.00         0\n",
            "         665       0.00      0.00      0.00         1\n",
            "         666       1.00      0.80      0.89         5\n",
            "         669       0.76      0.84      0.80       303\n",
            "         671       0.00      0.00      0.00         1\n",
            "         675       0.00      0.00      0.00         2\n",
            "         677       0.00      0.00      0.00         1\n",
            "         678       0.00      0.00      0.00         1\n",
            "         679       0.00      0.00      0.00         1\n",
            "         680       0.00      0.00      0.00         1\n",
            "         684       1.00      0.20      0.33         5\n",
            "         689       0.00      0.00      0.00         1\n",
            "         690       0.00      0.00      0.00         1\n",
            "         693       0.00      0.00      0.00         1\n",
            "         695       0.00      0.00      0.00         1\n",
            "         696       0.00      0.00      0.00         1\n",
            "         698       0.00      0.00      0.00         1\n",
            "         701       0.00      0.00      0.00         2\n",
            "         703       0.00      0.00      0.00         1\n",
            "         704       0.00      0.00      0.00         1\n",
            "         706       0.00      0.00      0.00         1\n",
            "         707       0.00      0.00      0.00         1\n",
            "         709       0.00      0.00      0.00         0\n",
            "         710       0.00      0.00      0.00         1\n",
            "         712       0.00      0.00      0.00         1\n",
            "         713       0.00      0.00      0.00         1\n",
            "         715       0.60      0.75      0.67         4\n",
            "         717       0.33      0.50      0.40         2\n",
            "         718       0.00      0.00      0.00         1\n",
            "         719       0.00      0.00      0.00         1\n",
            "         722       0.00      0.00      0.00         0\n",
            "         723       0.00      0.00      0.00         1\n",
            "         726       0.82      0.82      0.82        11\n",
            "         727       0.00      0.00      0.00         1\n",
            "         729       0.00      0.00      0.00         1\n",
            "         730       0.00      0.00      0.00         1\n",
            "         734       0.00      0.00      0.00         1\n",
            "         738       0.00      0.00      0.00         0\n",
            "         739       0.00      0.00      0.00         1\n",
            "         743       0.00      0.00      0.00         0\n",
            "         745       0.00      0.00      0.00         0\n",
            "         746       0.50      1.00      0.67         1\n",
            "         747       0.00      0.00      0.00         2\n",
            "         748       1.00      0.75      0.86         4\n",
            "         753       0.00      0.00      0.00         1\n",
            "         754       1.00      0.50      0.67         2\n",
            "         756       0.00      0.00      0.00         1\n",
            "         757       0.00      0.00      0.00         1\n",
            "         758       0.00      0.00      0.00         1\n",
            "         759       0.00      0.00      0.00         1\n",
            "         760       0.00      0.00      0.00         1\n",
            "         761       0.00      0.00      0.00         2\n",
            "         762       0.00      0.00      0.00         2\n",
            "         763       0.00      0.00      0.00         3\n",
            "         765       0.00      0.00      0.00         1\n",
            "         767       0.00      0.00      0.00         0\n",
            "         770       0.25      0.05      0.09        19\n",
            "         771       0.00      0.00      0.00         1\n",
            "         772       0.00      0.00      0.00         1\n",
            "         773       0.00      0.00      0.00         1\n",
            "         775       0.00      0.00      0.00         1\n",
            "         777       0.00      0.00      0.00         1\n",
            "         778       0.00      0.00      0.00         1\n",
            "         780       0.00      0.00      0.00         1\n",
            "         782       0.00      0.00      0.00         1\n",
            "         783       0.50      0.50      0.50         2\n",
            "         791       0.00      0.00      0.00         1\n",
            "         792       0.00      0.00      0.00         1\n",
            "         794       0.00      0.00      0.00         1\n",
            "         795       0.00      0.00      0.00         0\n",
            "         796       0.87      0.97      0.92      3637\n",
            "         797       0.60      0.75      0.67         4\n",
            "         800       0.33      0.25      0.29         4\n",
            "         801       0.00      0.00      0.00         0\n",
            "         803       0.00      0.00      0.00         1\n",
            "         805       1.00      0.50      0.67         4\n",
            "         806       0.00      0.00      0.00         1\n",
            "         807       0.00      0.00      0.00         1\n",
            "         808       0.00      0.00      0.00         1\n",
            "         811       0.00      0.00      0.00         1\n",
            "         816       1.00      1.00      1.00         1\n",
            "         817       0.55      0.58      0.56        19\n",
            "         819       0.00      0.00      0.00         0\n",
            "         820       0.00      0.00      0.00         1\n",
            "         825       0.00      0.00      0.00         1\n",
            "         826       0.00      0.00      0.00         0\n",
            "         828       0.58      0.32      0.42        34\n",
            "         834       0.00      0.00      0.00         1\n",
            "         840       0.50      0.14      0.22         7\n",
            "         845       0.00      0.00      0.00         1\n",
            "         847       0.00      0.00      0.00         1\n",
            "         849       0.00      0.00      0.00         1\n",
            "         850       0.00      0.00      0.00         3\n",
            "         851       0.40      1.00      0.57         2\n",
            "         854       0.00      0.00      0.00         9\n",
            "         855       0.00      0.00      0.00         1\n",
            "         856       0.00      0.00      0.00         1\n",
            "         857       0.00      0.00      0.00         1\n",
            "         860       1.00      1.00      1.00         1\n",
            "         861       0.00      0.00      0.00         0\n",
            "         862       0.00      0.00      0.00         1\n",
            "         864       0.00      0.00      0.00         1\n",
            "         865       0.00      0.00      0.00         1\n",
            "         866       0.67      1.00      0.80         2\n",
            "         867       0.00      0.00      0.00         0\n",
            "         868       0.00      0.00      0.00         1\n",
            "         872       1.00      0.20      0.33         5\n",
            "         874       0.00      0.00      0.00         1\n",
            "         875       0.27      0.40      0.32        10\n",
            "         877       1.00      1.00      1.00         1\n",
            "         878       0.00      0.00      0.00         1\n",
            "         879       0.00      0.00      0.00         1\n",
            "         881       0.00      0.00      0.00         2\n",
            "         882       0.00      0.00      0.00         3\n",
            "         884       0.50      0.33      0.40         3\n",
            "         885       0.00      0.00      0.00         1\n",
            "         886       0.00      0.00      0.00         0\n",
            "         887       0.00      0.00      0.00         1\n",
            "         888       0.00      0.00      0.00         1\n",
            "         890       1.00      0.50      0.67         2\n",
            "         891       0.60      0.49      0.54        63\n",
            "         894       0.00      0.00      0.00         1\n",
            "         897       0.00      0.00      0.00         1\n",
            "         898       0.00      0.00      0.00         1\n",
            "         899       0.00      0.00      0.00         0\n",
            "         902       0.00      0.00      0.00         1\n",
            "         905       0.00      0.00      0.00         1\n",
            "         906       0.00      0.00      0.00         1\n",
            "         910       0.00      0.00      0.00         1\n",
            "         912       0.00      0.00      0.00         1\n",
            "         913       0.00      0.00      0.00         2\n",
            "         916       1.00      1.00      1.00         1\n",
            "         918       0.00      0.00      0.00         0\n",
            "         919       0.00      0.00      0.00         1\n",
            "         920       0.00      0.00      0.00         1\n",
            "         924       0.00      0.00      0.00         1\n",
            "         925       0.00      0.00      0.00         1\n",
            "         926       0.43      0.30      0.35        10\n",
            "         930       0.00      0.00      0.00         3\n",
            "         931       0.00      0.00      0.00         2\n",
            "         933       0.00      0.00      0.00         2\n",
            "         936       0.00      0.00      0.00         1\n",
            "         940       0.00      0.00      0.00         0\n",
            "         941       0.00      0.00      0.00         2\n",
            "         943       0.00      0.00      0.00         0\n",
            "         944       0.00      0.00      0.00         1\n",
            "         946       0.00      0.00      0.00         1\n",
            "         948       0.00      0.00      0.00         1\n",
            "         951       0.00      0.00      0.00         2\n",
            "         952       0.00      0.00      0.00         0\n",
            "         953       0.00      0.00      0.00         1\n",
            "         958       0.00      0.00      0.00         1\n",
            "         959       0.50      0.67      0.57         3\n",
            "         961       0.00      0.00      0.00         1\n",
            "         962       0.00      0.00      0.00         1\n",
            "         964       0.00      0.00      0.00         1\n",
            "         965       0.00      0.00      0.00         1\n",
            "         968       0.00      0.00      0.00         1\n",
            "         969       0.00      0.00      0.00         1\n",
            "         970       0.00      0.00      0.00         2\n",
            "         972       0.00      0.00      0.00         2\n",
            "         973       0.00      0.00      0.00         1\n",
            "         974       0.00      0.00      0.00         3\n",
            "         976       0.50      0.20      0.29         5\n",
            "         982       0.00      0.00      0.00         2\n",
            "         984       0.00      0.00      0.00         2\n",
            "         986       0.00      0.00      0.00         1\n",
            "         987       0.50      0.13      0.21        15\n",
            "         992       0.00      0.00      0.00         1\n",
            "         993       0.00      0.00      0.00         1\n",
            "         999       0.00      0.00      0.00         1\n",
            "        1000       0.00      0.00      0.00         0\n",
            "        1001       0.00      0.00      0.00         1\n",
            "        1003       0.00      0.00      0.00         1\n",
            "        1004       0.64      0.67      0.65        21\n",
            "        1008       0.00      0.00      0.00         1\n",
            "        1009       0.00      0.00      0.00         1\n",
            "        1013       0.00      0.00      0.00         0\n",
            "        1016       0.00      0.00      0.00         1\n",
            "        1017       0.50      0.09      0.15        11\n",
            "        1018       0.00      0.00      0.00         2\n",
            "        1019       0.00      0.00      0.00         1\n",
            "        1020       0.00      0.00      0.00         1\n",
            "        1022       0.00      0.00      0.00         2\n",
            "        1024       0.00      0.00      0.00         1\n",
            "        1025       0.00      0.00      0.00         1\n",
            "        1026       1.00      0.57      0.73        14\n",
            "        1027       0.00      0.00      0.00         1\n",
            "        1034       0.40      0.50      0.44         4\n",
            "        1037       0.00      0.00      0.00         1\n",
            "        1038       0.73      0.89      0.80         9\n",
            "        1039       0.00      0.00      0.00         3\n",
            "        1042       0.00      0.00      0.00         1\n",
            "        1044       0.00      0.00      0.00         1\n",
            "        1046       0.00      0.00      0.00         1\n",
            "        1049       0.75      0.84      0.80       102\n",
            "        1050       0.00      0.00      0.00         3\n",
            "        1052       0.00      0.00      0.00         1\n",
            "        1053       0.00      0.00      0.00         1\n",
            "        1055       0.00      0.00      0.00         1\n",
            "        1058       0.00      0.00      0.00         1\n",
            "        1062       0.00      0.00      0.00         0\n",
            "        1064       0.00      0.00      0.00         0\n",
            "        1069       0.00      0.00      0.00         1\n",
            "        1070       0.00      0.00      0.00         1\n",
            "        1071       1.00      0.10      0.18        10\n",
            "        1075       0.00      0.00      0.00         1\n",
            "        1077       0.00      0.00      0.00         1\n",
            "        1078       0.00      0.00      0.00         1\n",
            "        1079       0.00      0.00      0.00         1\n",
            "        1083       0.00      0.00      0.00         0\n",
            "        1084       0.00      0.00      0.00         1\n",
            "        1085       0.00      0.00      0.00         1\n",
            "        1086       0.90      0.90      0.90        10\n",
            "        1087       0.00      0.00      0.00         1\n",
            "        1092       1.00      1.00      1.00         8\n",
            "        1096       1.00      0.80      0.89         5\n",
            "\n",
            "    accuracy                           0.81      7121\n",
            "   macro avg       0.18      0.17      0.17      7121\n",
            "weighted avg       0.77      0.81      0.79      7121\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Using Linear Support Vector Classification yieled an accuracy of ---> 0.8139306277208257\n",
            "Using support vector classification yielded a micro f1-score of ---> 0.8139306277208258\n",
            "Using support vector classification yielded a micro f1-score of ---> 0.1684871100729114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iihyQMlCdkMu"
      },
      "source": [
        "### Linear Support Vector Classification - Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a6x2JKBdkUc",
        "outputId": "ed4cda2e-6e60-4d44-cc10-64e61b8a298e"
      },
      "source": [
        "print('Available parameters to use for grid search----> ', lsvc_reut_model.get_params().keys())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Available parameters to use for grid search---->  dict_keys(['C', 'class_weight', 'dual', 'fit_intercept', 'intercept_scaling', 'loss', 'max_iter', 'multi_class', 'penalty', 'random_state', 'tol', 'verbose'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yOAxyXsdkai"
      },
      "source": [
        "parameters_lsvc_reut = {\n",
        "    'C' : (1, 2),\n",
        "    # 'penalty' : ('l1', 'l2'),\n",
        "    'multi_class' : ('ovr', 'crammer_singer'),\n",
        "    'loss' : ('hinge', 'squared_hinge')\n",
        "}"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyJmasJMhDde",
        "outputId": "bea0c572-304e-4afd-98a1-e584c9823be2"
      },
      "source": [
        "grid_search_lsvc_reut = GridSearchCV(lsvc_reut_model, parameters_lsvc_reut, cv=2, n_jobs=1, verbose=3)\n",
        "\n",
        "start = timer()\n",
        "grid_search_lsvc_reut = grid_search_lsvc_reut.fit(reut_x_train, reut_y_train)\n",
        "end = timer()\n",
        "print(\"Time taken to execute Grid search for Support Vector Classification!\", round(end-start))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
            "[CV] C=1, loss=hinge, multi_class=ovr ................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=2.\n",
            "  % (min_groups, self.n_splits)), UserWarning)\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] .... C=1, loss=hinge, multi_class=ovr, score=0.780, total=  18.6s\n",
            "[CV] C=1, loss=hinge, multi_class=ovr ................................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   18.6s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV] .... C=1, loss=hinge, multi_class=ovr, score=0.773, total=  14.9s\n",
            "[CV] C=1, loss=hinge, multi_class=crammer_singer .....................\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   33.5s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  C=1, loss=hinge, multi_class=crammer_singer, score=0.789, total=  43.6s\n",
            "[CV] C=1, loss=hinge, multi_class=crammer_singer .....................\n",
            "[CV]  C=1, loss=hinge, multi_class=crammer_singer, score=0.783, total=  44.7s\n",
            "[CV] C=1, loss=squared_hinge, multi_class=ovr ........................\n",
            "[CV]  C=1, loss=squared_hinge, multi_class=ovr, score=0.784, total=  12.0s\n",
            "[CV] C=1, loss=squared_hinge, multi_class=ovr ........................\n",
            "[CV]  C=1, loss=squared_hinge, multi_class=ovr, score=0.777, total=  12.0s\n",
            "[CV] C=1, loss=squared_hinge, multi_class=crammer_singer .............\n",
            "[CV]  C=1, loss=squared_hinge, multi_class=crammer_singer, score=0.789, total=  50.5s\n",
            "[CV] C=1, loss=squared_hinge, multi_class=crammer_singer .............\n",
            "[CV]  C=1, loss=squared_hinge, multi_class=crammer_singer, score=0.783, total= 1.2min\n",
            "[CV] C=2, loss=hinge, multi_class=ovr ................................\n",
            "[CV] .... C=2, loss=hinge, multi_class=ovr, score=0.788, total=  15.6s\n",
            "[CV] C=2, loss=hinge, multi_class=ovr ................................\n",
            "[CV] .... C=2, loss=hinge, multi_class=ovr, score=0.783, total=  14.7s\n",
            "[CV] C=2, loss=hinge, multi_class=crammer_singer .....................\n",
            "[CV]  C=2, loss=hinge, multi_class=crammer_singer, score=0.786, total= 1.0min\n",
            "[CV] C=2, loss=hinge, multi_class=crammer_singer .....................\n",
            "[CV]  C=2, loss=hinge, multi_class=crammer_singer, score=0.780, total= 1.9min\n",
            "[CV] C=2, loss=squared_hinge, multi_class=ovr ........................\n",
            "[CV]  C=2, loss=squared_hinge, multi_class=ovr, score=0.784, total=  13.6s\n",
            "[CV] C=2, loss=squared_hinge, multi_class=ovr ........................\n",
            "[CV]  C=2, loss=squared_hinge, multi_class=ovr, score=0.778, total=  13.9s\n",
            "[CV] C=2, loss=squared_hinge, multi_class=crammer_singer .............\n",
            "[CV]  C=2, loss=squared_hinge, multi_class=crammer_singer, score=0.786, total=  58.4s\n",
            "[CV] C=2, loss=squared_hinge, multi_class=crammer_singer .............\n",
            "[CV]  C=2, loss=squared_hinge, multi_class=crammer_singer, score=0.780, total= 1.6min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed: 11.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time taken to execute Grid search for Support Vector Classification! 897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqbbQOaShDgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eff41163-4595-409b-c7ba-4fc1081708c5"
      },
      "source": [
        "print(\"Best Score:\", grid_search_lsvc_reut.best_score_)\n",
        "print()\n",
        "print(\"Best Parameters\", grid_search_lsvc_reut.best_params_)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Score: 0.7860549818442488\n",
            "\n",
            "Best Parameters {'C': 1, 'loss': 'hinge', 'multi_class': 'crammer_singer'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_9vPg0Biadd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d04aa7-0b34-4cbe-f8aa-bc9a0c4c3fb2"
      },
      "source": [
        "grid_search_lsvc_reut_y_pred = grid_search_lsvc_reut.predict(reut_x_test)\n",
        "print('SCORES WITH RANDOM FOREST CLASSIFIER AFTER GRID SEARCH')\n",
        "print('\\n\\n')\n",
        "print(classification_report(reut_y_test, grid_search_lsvc_reut_y_pred, zero_division='warn'))\n",
        "print('\\n\\n')\n",
        "print(\"Using random forest classifier yieled an accuracy of --->\", grid_search_lsvc_reut.score(reut_x_test, reut_y_test))\n",
        "print(\"Using random forest classifier a micro f1-score of --->\", f1_score(reut_y_test, grid_search_lsvc_reut_y_pred, average='micro'))\n",
        "print(\"Using random forest classifier a macro f1-score of --->\", f1_score(reut_y_test, grid_search_lsvc_reut_y_pred, average='macro'))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SCORES WITH RANDOM FOREST CLASSIFIER AFTER GRID SEARCH\n",
            "\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.88      0.86       884\n",
            "           3       0.00      0.00      0.00         1\n",
            "           4       0.00      0.00      0.00         2\n",
            "           6       0.00      0.00      0.00         0\n",
            "           7       0.88      0.58      0.70        12\n",
            "           8       0.00      0.00      0.00         2\n",
            "           9       0.00      0.00      0.00         0\n",
            "          10       0.00      0.00      0.00         1\n",
            "          11       0.00      0.00      0.00         0\n",
            "          12       0.00      0.00      0.00         1\n",
            "          13       1.00      1.00      1.00         1\n",
            "          14       0.00      0.00      0.00         1\n",
            "          15       0.78      0.87      0.82        60\n",
            "          16       0.00      0.00      0.00         1\n",
            "          19       0.00      0.00      0.00         0\n",
            "          20       0.00      0.00      0.00         0\n",
            "          21       0.00      0.00      0.00         1\n",
            "          22       0.00      0.00      0.00         1\n",
            "          24       0.00      0.00      0.00         1\n",
            "          25       0.00      0.00      0.00         1\n",
            "          26       1.00      0.50      0.67         4\n",
            "          28       0.00      0.00      0.00         1\n",
            "          30       0.75      0.75      0.75         4\n",
            "          31       0.00      0.00      0.00         0\n",
            "          33       0.00      0.00      0.00         1\n",
            "          36       0.00      0.00      0.00         1\n",
            "          37       0.33      0.50      0.40         2\n",
            "          41       0.80      0.89      0.84         9\n",
            "          42       0.00      0.00      0.00         1\n",
            "          45       0.00      0.00      0.00         0\n",
            "          46       0.57      0.65      0.61        43\n",
            "          47       0.00      0.00      0.00         0\n",
            "          48       0.00      0.00      0.00         1\n",
            "          49       0.00      0.00      0.00         0\n",
            "          50       0.00      0.00      0.00         2\n",
            "          55       0.00      0.00      0.00         1\n",
            "          57       0.00      0.00      0.00         0\n",
            "          60       0.00      0.00      0.00         1\n",
            "          61       0.00      0.00      0.00         1\n",
            "          63       0.00      0.00      0.00         1\n",
            "          64       0.00      0.00      0.00         1\n",
            "          65       0.00      0.00      0.00         0\n",
            "          66       0.00      0.00      0.00         1\n",
            "          67       0.00      0.00      0.00         1\n",
            "          69       0.00      0.00      0.00         1\n",
            "          71       0.00      0.00      0.00         1\n",
            "          74       1.00      0.50      0.67         2\n",
            "          77       0.00      0.00      0.00         0\n",
            "          78       0.00      0.00      0.00         1\n",
            "          79       0.00      0.00      0.00         1\n",
            "          82       0.00      0.00      0.00         0\n",
            "          83       0.00      0.00      0.00         1\n",
            "          84       0.00      0.00      0.00         1\n",
            "          85       0.00      0.00      0.00         2\n",
            "          86       1.00      0.80      0.89         5\n",
            "          89       0.00      0.00      0.00         1\n",
            "          91       0.76      0.87      0.81        45\n",
            "          93       0.00      0.00      0.00         1\n",
            "          94       0.00      0.00      0.00         0\n",
            "          97       0.00      0.00      0.00         1\n",
            "          98       0.00      0.00      0.00         1\n",
            "         100       0.00      0.00      0.00         1\n",
            "         103       0.00      0.00      0.00         3\n",
            "         106       0.00      0.00      0.00         4\n",
            "         107       0.00      0.00      0.00         1\n",
            "         110       1.00      1.00      1.00         1\n",
            "         111       0.00      0.00      0.00         0\n",
            "         112       0.00      0.00      0.00         1\n",
            "         113       0.77      0.55      0.64       289\n",
            "         114       0.00      0.00      0.00         1\n",
            "         116       0.00      0.00      0.00         1\n",
            "         118       0.00      0.00      0.00         1\n",
            "         119       0.00      0.00      0.00         1\n",
            "         122       0.40      0.33      0.36         6\n",
            "         124       0.00      0.00      0.00         1\n",
            "         127       0.00      0.00      0.00         1\n",
            "         130       0.00      0.00      0.00         2\n",
            "         131       0.00      0.00      0.00        12\n",
            "         133       0.00      0.00      0.00         1\n",
            "         135       0.00      0.00      0.00         1\n",
            "         137       0.00      0.00      0.00         2\n",
            "         138       0.71      0.89      0.79        28\n",
            "         139       0.00      0.00      0.00         1\n",
            "         142       0.00      0.00      0.00         1\n",
            "         143       0.00      0.00      0.00         1\n",
            "         145       0.00      0.00      0.00         1\n",
            "         146       0.00      0.00      0.00         1\n",
            "         147       0.50      1.00      0.67         1\n",
            "         149       0.00      0.00      0.00         1\n",
            "         150       0.00      0.00      0.00         1\n",
            "         152       0.00      0.00      0.00         2\n",
            "         153       0.00      0.00      0.00         1\n",
            "         156       0.83      1.00      0.91         5\n",
            "         157       0.00      0.00      0.00         1\n",
            "         161       0.50      0.33      0.40         3\n",
            "         162       0.00      0.00      0.00         1\n",
            "         164       1.00      0.40      0.57         5\n",
            "         165       1.00      1.00      1.00         1\n",
            "         167       0.00      0.00      0.00         2\n",
            "         169       0.00      0.00      0.00         1\n",
            "         171       0.00      0.00      0.00         1\n",
            "         173       0.00      0.00      0.00         0\n",
            "         174       0.00      0.00      0.00         1\n",
            "         176       0.83      0.62      0.71         8\n",
            "         177       0.00      0.00      0.00         1\n",
            "         178       0.00      0.00      0.00         1\n",
            "         179       0.00      0.00      0.00         1\n",
            "         180       0.00      0.00      0.00         1\n",
            "         182       0.75      0.92      0.83        13\n",
            "         183       0.00      0.00      0.00         0\n",
            "         186       0.00      0.00      0.00         1\n",
            "         187       1.00      1.00      1.00         1\n",
            "         188       0.00      0.00      0.00         1\n",
            "         189       0.00      0.00      0.00         1\n",
            "         191       0.00      0.00      0.00         1\n",
            "         193       1.00      1.00      1.00         1\n",
            "         196       0.00      0.00      0.00         0\n",
            "         198       0.00      0.00      0.00         1\n",
            "         199       0.75      0.86      0.80         7\n",
            "         200       0.00      0.00      0.00         1\n",
            "         203       0.00      0.00      0.00         1\n",
            "         206       0.66      0.77      0.71        97\n",
            "         207       0.00      0.00      0.00         1\n",
            "         208       0.00      0.00      0.00         1\n",
            "         210       0.00      0.00      0.00         1\n",
            "         211       0.00      0.00      0.00         1\n",
            "         212       0.00      0.00      0.00         0\n",
            "         218       0.00      0.00      0.00         0\n",
            "         220       0.00      0.00      0.00         1\n",
            "         221       0.00      0.00      0.00         1\n",
            "         229       1.00      1.00      1.00         1\n",
            "         230       0.00      0.00      0.00         1\n",
            "         232       0.00      0.00      0.00         1\n",
            "         233       0.00      0.00      0.00         1\n",
            "         236       0.00      0.00      0.00         1\n",
            "         237       1.00      1.00      1.00         1\n",
            "         242       0.00      0.00      0.00         3\n",
            "         245       0.00      0.00      0.00         1\n",
            "         247       0.00      0.00      0.00         0\n",
            "         248       1.00      1.00      1.00         2\n",
            "         250       0.00      0.00      0.00         2\n",
            "         255       0.00      0.00      0.00         2\n",
            "         256       0.00      0.00      0.00         1\n",
            "         257       1.00      1.00      1.00         1\n",
            "         259       0.50      0.50      0.50         2\n",
            "         260       0.00      0.00      0.00         1\n",
            "         261       0.00      0.00      0.00         1\n",
            "         262       0.00      0.00      0.00         2\n",
            "         263       0.00      0.00      0.00         1\n",
            "         264       0.00      0.00      0.00         0\n",
            "         265       0.75      0.75      0.75        24\n",
            "         267       0.00      0.00      0.00         1\n",
            "         270       0.00      0.00      0.00         2\n",
            "         271       0.00      0.00      0.00         1\n",
            "         273       0.00      0.00      0.00         1\n",
            "         274       0.00      0.00      0.00         2\n",
            "         277       0.00      0.00      0.00         1\n",
            "         279       0.00      0.00      0.00         1\n",
            "         281       0.00      0.00      0.00         1\n",
            "         283       0.00      0.00      0.00         1\n",
            "         284       1.00      1.00      1.00         1\n",
            "         287       1.00      0.75      0.86         4\n",
            "         288       0.00      0.00      0.00         1\n",
            "         289       0.57      0.67      0.62        12\n",
            "         292       1.00      1.00      1.00         1\n",
            "         294       0.00      0.00      0.00         1\n",
            "         295       0.00      0.00      0.00         0\n",
            "         297       0.73      0.95      0.83        20\n",
            "         300       0.00      0.00      0.00         1\n",
            "         301       0.00      0.00      0.00         1\n",
            "         302       0.00      0.00      0.00         1\n",
            "         304       0.00      0.00      0.00         1\n",
            "         305       0.00      0.00      0.00         1\n",
            "         307       0.00      0.00      0.00         2\n",
            "         310       0.00      0.00      0.00         0\n",
            "         313       0.71      0.45      0.56        11\n",
            "         314       0.00      0.00      0.00         1\n",
            "         315       0.00      0.00      0.00         1\n",
            "         316       0.00      0.00      0.00         2\n",
            "         318       0.00      0.00      0.00         0\n",
            "         319       0.00      0.00      0.00         1\n",
            "         320       0.00      0.00      0.00         0\n",
            "         322       0.00      0.00      0.00         1\n",
            "         323       0.00      0.00      0.00         1\n",
            "         326       0.00      0.00      0.00         0\n",
            "         327       0.00      0.00      0.00         1\n",
            "         329       0.00      0.00      0.00         0\n",
            "         331       0.78      0.78      0.78        36\n",
            "         336       0.00      0.00      0.00         1\n",
            "         337       0.00      0.00      0.00         1\n",
            "         340       0.00      0.00      0.00         1\n",
            "         341       0.00      0.00      0.00         2\n",
            "         342       0.00      0.00      0.00         1\n",
            "         343       0.00      0.00      0.00         1\n",
            "         344       0.00      0.00      0.00         0\n",
            "         346       0.43      0.75      0.55         4\n",
            "         348       0.00      0.00      0.00         0\n",
            "         349       0.82      0.82      0.82       190\n",
            "         351       1.00      0.50      0.67         2\n",
            "         354       1.00      1.00      1.00         1\n",
            "         355       0.00      0.00      0.00         0\n",
            "         357       0.33      0.33      0.33         3\n",
            "         363       0.00      0.00      0.00         0\n",
            "         365       0.00      0.00      0.00         1\n",
            "         367       0.00      0.00      0.00         1\n",
            "         371       0.00      0.00      0.00         0\n",
            "         372       0.00      0.00      0.00         1\n",
            "         374       0.00      0.00      0.00         1\n",
            "         378       0.00      0.00      0.00         0\n",
            "         379       0.00      0.00      0.00         1\n",
            "         381       0.00      0.00      0.00         1\n",
            "         383       0.00      0.00      0.00         1\n",
            "         386       0.00      0.00      0.00         1\n",
            "         387       0.00      0.00      0.00         1\n",
            "         389       0.00      0.00      0.00         2\n",
            "         391       0.00      0.00      0.00         0\n",
            "         392       0.00      0.00      0.00         1\n",
            "         393       0.00      0.00      0.00         1\n",
            "         394       0.00      0.00      0.00         3\n",
            "         395       0.33      1.00      0.50         1\n",
            "         397       0.00      0.00      0.00         1\n",
            "         399       0.17      0.18      0.18        28\n",
            "         400       0.00      0.00      0.00         1\n",
            "         401       0.00      0.00      0.00         0\n",
            "         404       0.00      0.00      0.00         1\n",
            "         405       0.00      0.00      0.00         0\n",
            "         407       0.00      0.00      0.00         1\n",
            "         412       1.00      1.00      1.00         1\n",
            "         414       0.00      0.00      0.00         1\n",
            "         416       1.00      1.00      1.00         2\n",
            "         417       0.00      0.00      0.00         1\n",
            "         418       0.67      0.67      0.67         3\n",
            "         419       0.00      0.00      0.00         1\n",
            "         422       0.67      0.50      0.57         4\n",
            "         423       0.00      0.00      0.00         1\n",
            "         425       0.00      0.00      0.00         1\n",
            "         428       0.00      0.00      0.00         1\n",
            "         429       0.00      0.00      0.00         0\n",
            "         431       0.00      0.00      0.00         1\n",
            "         437       0.25      0.33      0.29         6\n",
            "         438       0.00      0.00      0.00         1\n",
            "         440       0.00      0.00      0.00         1\n",
            "         442       0.00      0.00      0.00         1\n",
            "         445       0.00      0.00      0.00         1\n",
            "         446       0.00      0.00      0.00         1\n",
            "         449       0.00      0.00      0.00         1\n",
            "         450       0.00      0.00      0.00         2\n",
            "         451       1.00      0.50      0.67         2\n",
            "         452       0.00      0.00      0.00         1\n",
            "         453       0.79      1.00      0.88        15\n",
            "         454       0.00      0.00      0.00         1\n",
            "         456       0.00      0.00      0.00         1\n",
            "         458       0.00      0.00      0.00         1\n",
            "         460       0.00      0.00      0.00         0\n",
            "         461       0.00      0.00      0.00         2\n",
            "         462       0.00      0.00      0.00         0\n",
            "         463       0.00      0.00      0.00         1\n",
            "         465       0.00      0.00      0.00         1\n",
            "         466       1.00      1.00      1.00         1\n",
            "         467       0.00      0.00      0.00         0\n",
            "         468       0.00      0.00      0.00         2\n",
            "         469       0.00      0.00      0.00         1\n",
            "         470       0.00      0.00      0.00         1\n",
            "         472       0.50      1.00      0.67         1\n",
            "         473       0.00      0.00      0.00         1\n",
            "         475       0.86      0.86      0.86        50\n",
            "         478       0.00      0.00      0.00         1\n",
            "         479       0.00      0.00      0.00         1\n",
            "         480       0.00      0.00      0.00         0\n",
            "         484       0.00      0.00      0.00         1\n",
            "         487       0.00      0.00      0.00         1\n",
            "         488       0.00      0.00      0.00         3\n",
            "         489       0.00      0.00      0.00         1\n",
            "         491       0.90      0.79      0.84        34\n",
            "         492       1.00      1.00      1.00         1\n",
            "         494       1.00      1.00      1.00         1\n",
            "         495       0.00      0.00      0.00         1\n",
            "         496       0.00      0.00      0.00         2\n",
            "         498       0.50      1.00      0.67         1\n",
            "         499       0.00      0.00      0.00         1\n",
            "         500       0.83      0.71      0.77         7\n",
            "         502       0.00      0.00      0.00         1\n",
            "         503       0.00      0.00      0.00         1\n",
            "         504       0.00      0.00      0.00         1\n",
            "         505       0.00      0.00      0.00         1\n",
            "         506       0.00      0.00      0.00         1\n",
            "         507       0.75      0.75      0.75         8\n",
            "         510       1.00      1.00      1.00         1\n",
            "         513       0.60      0.60      0.60         5\n",
            "         514       0.50      1.00      0.67         1\n",
            "         516       0.00      0.00      0.00         1\n",
            "         518       0.00      0.00      0.00         0\n",
            "         521       0.89      0.80      0.84        10\n",
            "         522       0.50      1.00      0.67         1\n",
            "         523       0.65      0.76      0.70        17\n",
            "         525       0.00      0.00      0.00         1\n",
            "         527       0.00      0.00      0.00         1\n",
            "         528       0.00      0.00      0.00         1\n",
            "         529       0.00      0.00      0.00         0\n",
            "         530       0.00      0.00      0.00         1\n",
            "         531       0.00      0.00      0.00         2\n",
            "         532       0.00      0.00      0.00         1\n",
            "         534       1.00      0.33      0.50         3\n",
            "         535       0.00      0.00      0.00         0\n",
            "         536       0.60      0.60      0.60         5\n",
            "         539       0.56      0.71      0.63         7\n",
            "         541       0.25      1.00      0.40         1\n",
            "         542       0.00      0.00      0.00         1\n",
            "         545       0.77      1.00      0.87        10\n",
            "         547       0.00      0.00      0.00         1\n",
            "         548       0.00      0.00      0.00         2\n",
            "         549       0.00      0.00      0.00         1\n",
            "         551       0.00      0.00      0.00         1\n",
            "         552       0.92      0.88      0.90        25\n",
            "         553       0.00      0.00      0.00         2\n",
            "         554       0.69      0.69      0.69        13\n",
            "         557       0.00      0.00      0.00         0\n",
            "         558       0.67      0.67      0.67         3\n",
            "         559       0.00      0.00      0.00         1\n",
            "         560       0.71      0.94      0.81        18\n",
            "         564       0.00      0.00      0.00         1\n",
            "         565       0.00      0.00      0.00         1\n",
            "         566       0.38      0.50      0.43         6\n",
            "         569       0.00      0.00      0.00         2\n",
            "         570       0.33      1.00      0.50         1\n",
            "         571       0.00      0.00      0.00         0\n",
            "         573       0.52      0.73      0.61        15\n",
            "         577       0.00      0.00      0.00         1\n",
            "         585       0.00      0.00      0.00         7\n",
            "         587       0.70      0.88      0.78        49\n",
            "         592       0.00      0.00      0.00         3\n",
            "         593       0.00      0.00      0.00         1\n",
            "         594       0.00      0.00      0.00         1\n",
            "         596       0.00      0.00      0.00         1\n",
            "         598       0.00      0.00      0.00         1\n",
            "         600       0.00      0.00      0.00         0\n",
            "         601       0.00      0.00      0.00         1\n",
            "         603       0.00      0.00      0.00         1\n",
            "         607       1.00      1.00      1.00         1\n",
            "         609       0.00      0.00      0.00         2\n",
            "         613       0.65      0.69      0.67        16\n",
            "         615       0.00      0.00      0.00         1\n",
            "         616       0.00      0.00      0.00         1\n",
            "         617       0.00      0.00      0.00         1\n",
            "         622       0.00      0.00      0.00         1\n",
            "         623       0.67      0.57      0.62         7\n",
            "         624       0.00      0.00      0.00         1\n",
            "         626       0.60      1.00      0.75         3\n",
            "         627       1.00      1.00      1.00         1\n",
            "         628       0.00      0.00      0.00         1\n",
            "         629       0.75      0.75      0.75        12\n",
            "         630       0.00      0.00      0.00         0\n",
            "         632       0.00      0.00      0.00         0\n",
            "         634       0.00      0.00      0.00         1\n",
            "         637       0.00      0.00      0.00         1\n",
            "         638       0.00      0.00      0.00         0\n",
            "         639       0.00      0.00      0.00         1\n",
            "         641       0.00      0.00      0.00         1\n",
            "         643       0.00      0.00      0.00         1\n",
            "         645       0.00      0.00      0.00         1\n",
            "         646       0.62      0.83      0.71         6\n",
            "         647       1.00      1.00      1.00         1\n",
            "         648       0.50      1.00      0.67         1\n",
            "         650       0.00      0.00      0.00         1\n",
            "         651       0.00      0.00      0.00         1\n",
            "         653       0.62      0.71      0.67         7\n",
            "         654       0.00      0.00      0.00         1\n",
            "         657       0.00      0.00      0.00         1\n",
            "         658       0.00      0.00      0.00         1\n",
            "         664       0.00      0.00      0.00         0\n",
            "         665       0.00      0.00      0.00         1\n",
            "         666       1.00      0.80      0.89         5\n",
            "         669       0.77      0.84      0.80       303\n",
            "         671       0.00      0.00      0.00         1\n",
            "         675       0.50      0.50      0.50         2\n",
            "         677       0.00      0.00      0.00         1\n",
            "         678       0.00      0.00      0.00         1\n",
            "         679       0.00      0.00      0.00         1\n",
            "         680       0.00      0.00      0.00         1\n",
            "         684       1.00      0.20      0.33         5\n",
            "         689       0.00      0.00      0.00         1\n",
            "         690       0.00      0.00      0.00         1\n",
            "         693       0.00      0.00      0.00         1\n",
            "         695       0.00      0.00      0.00         1\n",
            "         696       0.00      0.00      0.00         1\n",
            "         698       0.00      0.00      0.00         1\n",
            "         701       0.00      0.00      0.00         2\n",
            "         703       0.00      0.00      0.00         1\n",
            "         704       0.00      0.00      0.00         1\n",
            "         706       0.00      0.00      0.00         1\n",
            "         707       0.00      0.00      0.00         1\n",
            "         709       0.00      0.00      0.00         0\n",
            "         710       0.00      0.00      0.00         1\n",
            "         712       0.00      0.00      0.00         1\n",
            "         713       0.00      0.00      0.00         1\n",
            "         715       0.60      0.75      0.67         4\n",
            "         717       0.33      0.50      0.40         2\n",
            "         718       0.00      0.00      0.00         1\n",
            "         719       0.00      0.00      0.00         1\n",
            "         722       0.00      0.00      0.00         0\n",
            "         723       0.00      0.00      0.00         1\n",
            "         726       0.69      0.82      0.75        11\n",
            "         727       0.00      0.00      0.00         1\n",
            "         729       0.00      0.00      0.00         1\n",
            "         730       0.00      0.00      0.00         1\n",
            "         734       0.00      0.00      0.00         1\n",
            "         738       0.00      0.00      0.00         0\n",
            "         739       0.00      0.00      0.00         1\n",
            "         743       0.00      0.00      0.00         0\n",
            "         745       0.00      0.00      0.00         0\n",
            "         746       0.33      1.00      0.50         1\n",
            "         747       0.00      0.00      0.00         2\n",
            "         748       1.00      0.75      0.86         4\n",
            "         753       0.00      0.00      0.00         1\n",
            "         754       1.00      0.50      0.67         2\n",
            "         756       0.00      0.00      0.00         1\n",
            "         757       0.00      0.00      0.00         1\n",
            "         758       0.00      0.00      0.00         1\n",
            "         759       0.00      0.00      0.00         1\n",
            "         760       0.00      0.00      0.00         1\n",
            "         761       0.00      0.00      0.00         2\n",
            "         762       0.00      0.00      0.00         2\n",
            "         763       0.00      0.00      0.00         3\n",
            "         765       0.00      0.00      0.00         1\n",
            "         767       0.00      0.00      0.00         0\n",
            "         770       0.25      0.05      0.09        19\n",
            "         771       0.00      0.00      0.00         1\n",
            "         772       0.00      0.00      0.00         1\n",
            "         773       0.00      0.00      0.00         1\n",
            "         775       0.00      0.00      0.00         1\n",
            "         777       0.00      0.00      0.00         1\n",
            "         778       0.00      0.00      0.00         1\n",
            "         780       0.00      0.00      0.00         1\n",
            "         782       0.00      0.00      0.00         1\n",
            "         783       0.50      0.50      0.50         2\n",
            "         788       0.00      0.00      0.00         0\n",
            "         791       0.00      0.00      0.00         1\n",
            "         792       0.00      0.00      0.00         1\n",
            "         794       0.00      0.00      0.00         1\n",
            "         795       0.00      0.00      0.00         0\n",
            "         796       0.89      0.97      0.93      3637\n",
            "         797       0.43      0.75      0.55         4\n",
            "         800       0.33      0.25      0.29         4\n",
            "         803       0.00      0.00      0.00         1\n",
            "         805       0.50      0.50      0.50         4\n",
            "         806       0.00      0.00      0.00         1\n",
            "         807       0.00      0.00      0.00         1\n",
            "         808       0.00      0.00      0.00         1\n",
            "         811       0.00      0.00      0.00         1\n",
            "         816       1.00      1.00      1.00         1\n",
            "         817       0.52      0.58      0.55        19\n",
            "         819       0.00      0.00      0.00         0\n",
            "         820       0.00      0.00      0.00         1\n",
            "         825       0.00      0.00      0.00         1\n",
            "         826       0.00      0.00      0.00         0\n",
            "         828       0.62      0.38      0.47        34\n",
            "         833       0.00      0.00      0.00         0\n",
            "         834       0.00      0.00      0.00         1\n",
            "         840       0.25      0.14      0.18         7\n",
            "         843       0.00      0.00      0.00         0\n",
            "         845       0.00      0.00      0.00         1\n",
            "         847       0.00      0.00      0.00         1\n",
            "         849       0.00      0.00      0.00         1\n",
            "         850       0.00      0.00      0.00         3\n",
            "         851       0.33      1.00      0.50         2\n",
            "         854       0.00      0.00      0.00         9\n",
            "         855       0.00      0.00      0.00         1\n",
            "         856       0.00      0.00      0.00         1\n",
            "         857       0.00      0.00      0.00         1\n",
            "         860       1.00      1.00      1.00         1\n",
            "         861       0.00      0.00      0.00         0\n",
            "         862       0.00      0.00      0.00         1\n",
            "         864       0.00      0.00      0.00         1\n",
            "         865       0.00      0.00      0.00         1\n",
            "         866       0.67      1.00      0.80         2\n",
            "         867       0.00      0.00      0.00         0\n",
            "         868       0.00      0.00      0.00         1\n",
            "         872       1.00      0.20      0.33         5\n",
            "         874       0.00      0.00      0.00         1\n",
            "         875       0.29      0.40      0.33        10\n",
            "         877       1.00      1.00      1.00         1\n",
            "         878       0.00      0.00      0.00         1\n",
            "         879       0.00      0.00      0.00         1\n",
            "         881       0.00      0.00      0.00         2\n",
            "         882       0.33      0.33      0.33         3\n",
            "         884       0.50      0.33      0.40         3\n",
            "         885       0.00      0.00      0.00         1\n",
            "         886       0.00      0.00      0.00         0\n",
            "         887       0.00      0.00      0.00         1\n",
            "         888       0.00      0.00      0.00         1\n",
            "         890       1.00      0.50      0.67         2\n",
            "         891       0.60      0.56      0.58        63\n",
            "         894       0.00      0.00      0.00         1\n",
            "         897       0.00      0.00      0.00         1\n",
            "         898       0.00      0.00      0.00         1\n",
            "         899       0.00      0.00      0.00         0\n",
            "         902       0.00      0.00      0.00         1\n",
            "         905       0.00      0.00      0.00         1\n",
            "         906       0.00      0.00      0.00         1\n",
            "         910       0.00      0.00      0.00         1\n",
            "         912       0.00      0.00      0.00         1\n",
            "         913       0.00      0.00      0.00         2\n",
            "         916       1.00      1.00      1.00         1\n",
            "         918       0.00      0.00      0.00         0\n",
            "         919       0.00      0.00      0.00         1\n",
            "         920       0.00      0.00      0.00         1\n",
            "         924       0.00      0.00      0.00         1\n",
            "         925       0.00      0.00      0.00         1\n",
            "         926       0.50      0.40      0.44        10\n",
            "         930       0.00      0.00      0.00         3\n",
            "         931       0.00      0.00      0.00         2\n",
            "         933       0.00      0.00      0.00         2\n",
            "         936       0.00      0.00      0.00         1\n",
            "         940       0.00      0.00      0.00         0\n",
            "         941       0.00      0.00      0.00         2\n",
            "         943       0.00      0.00      0.00         0\n",
            "         944       0.00      0.00      0.00         1\n",
            "         946       0.00      0.00      0.00         1\n",
            "         948       0.00      0.00      0.00         1\n",
            "         949       0.00      0.00      0.00         0\n",
            "         951       0.00      0.00      0.00         2\n",
            "         952       0.00      0.00      0.00         0\n",
            "         953       0.00      0.00      0.00         1\n",
            "         958       0.00      0.00      0.00         1\n",
            "         959       0.43      1.00      0.60         3\n",
            "         961       0.00      0.00      0.00         1\n",
            "         962       0.00      0.00      0.00         1\n",
            "         964       0.00      0.00      0.00         1\n",
            "         965       0.00      0.00      0.00         1\n",
            "         968       0.00      0.00      0.00         1\n",
            "         969       0.00      0.00      0.00         1\n",
            "         970       0.00      0.00      0.00         2\n",
            "         972       0.00      0.00      0.00         2\n",
            "         973       0.00      0.00      0.00         1\n",
            "         974       0.00      0.00      0.00         3\n",
            "         976       0.00      0.00      0.00         5\n",
            "         981       0.00      0.00      0.00         0\n",
            "         982       0.00      0.00      0.00         2\n",
            "         984       0.00      0.00      0.00         2\n",
            "         986       0.00      0.00      0.00         1\n",
            "         987       0.40      0.13      0.20        15\n",
            "         992       0.00      0.00      0.00         1\n",
            "         993       0.00      0.00      0.00         1\n",
            "         999       0.00      0.00      0.00         1\n",
            "        1000       0.00      0.00      0.00         0\n",
            "        1001       0.00      0.00      0.00         1\n",
            "        1003       0.00      0.00      0.00         1\n",
            "        1004       0.64      0.67      0.65        21\n",
            "        1008       0.00      0.00      0.00         1\n",
            "        1009       0.00      0.00      0.00         1\n",
            "        1013       0.00      0.00      0.00         0\n",
            "        1016       0.00      0.00      0.00         1\n",
            "        1017       0.50      0.09      0.15        11\n",
            "        1018       0.00      0.00      0.00         2\n",
            "        1019       0.00      0.00      0.00         1\n",
            "        1020       0.00      0.00      0.00         1\n",
            "        1021       0.00      0.00      0.00         0\n",
            "        1022       0.00      0.00      0.00         2\n",
            "        1024       0.00      0.00      0.00         1\n",
            "        1025       0.00      0.00      0.00         1\n",
            "        1026       1.00      0.57      0.73        14\n",
            "        1027       0.00      0.00      0.00         1\n",
            "        1034       0.40      0.50      0.44         4\n",
            "        1037       0.00      0.00      0.00         1\n",
            "        1038       0.73      0.89      0.80         9\n",
            "        1039       0.00      0.00      0.00         3\n",
            "        1042       0.00      0.00      0.00         1\n",
            "        1044       0.00      0.00      0.00         1\n",
            "        1046       0.00      0.00      0.00         1\n",
            "        1049       0.75      0.84      0.80       102\n",
            "        1050       0.00      0.00      0.00         3\n",
            "        1052       0.00      0.00      0.00         1\n",
            "        1053       0.00      0.00      0.00         1\n",
            "        1055       0.00      0.00      0.00         1\n",
            "        1058       0.00      0.00      0.00         1\n",
            "        1062       0.00      0.00      0.00         0\n",
            "        1064       0.00      0.00      0.00         0\n",
            "        1069       0.00      0.00      0.00         1\n",
            "        1070       0.00      0.00      0.00         1\n",
            "        1071       1.00      0.10      0.18        10\n",
            "        1075       0.00      0.00      0.00         1\n",
            "        1077       0.00      0.00      0.00         1\n",
            "        1078       0.00      0.00      0.00         1\n",
            "        1079       0.00      0.00      0.00         1\n",
            "        1083       0.00      0.00      0.00         0\n",
            "        1084       0.00      0.00      0.00         1\n",
            "        1085       0.00      0.00      0.00         1\n",
            "        1086       0.90      0.90      0.90        10\n",
            "        1087       0.00      0.00      0.00         1\n",
            "        1090       0.00      0.00      0.00         0\n",
            "        1092       1.00      0.88      0.93         8\n",
            "        1093       0.00      0.00      0.00         0\n",
            "        1096       1.00      0.80      0.89         5\n",
            "\n",
            "    accuracy                           0.82      7121\n",
            "   macro avg       0.17      0.17      0.16      7121\n",
            "weighted avg       0.78      0.82      0.79      7121\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Usireut multinomial naive bayes yieled an accuracy of ---> 0.8171605111641623\n",
            "Usireut multinomial naive bayes a micro f1-score of ---> 0.8171605111641623\n",
            "Usireut multinomial naive bayes a macro f1-score of ---> 0.15974942196021472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSqxsuUvWOwx"
      },
      "source": [
        "# Design Choices and Qualitative Analysis "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szEwk--DWVL3"
      },
      "source": [
        "The two datasets used for this machine learning model were parsed quite differently. The newsgroup dataset was easily parsed through sklearn's method whereas the Reuters required the use of regular expressions and beautiful soup.\n",
        "\n",
        "The Random Forest Classifieri was utilized as a probabilistic model whilst the Linear Support Vector Classifier was used as a non-probablistic model, for both datasets. Given the larger size of the Reuters dataset, stopwords and the n-gram were removed from the encoding when working with said dataset. This decision was taken after noticing the computational time needed to grid search for the best hyperparameters of the model. On the other hand, stopwords, n-grams, idf and the norm parameter were implemented on the encoding for the newsgroup dataset. From this point forward, both of these structures will be referred to as 'base model'. Furhermore, the maximum depth of the tree for the RFC was set to 10 to reduce the computational time needed to fit the model. In any scenario, expanding the nodes until all leaves are pure would yield the best possible result however this needed to be sacrified to balance out the time needed.\n",
        "\n",
        "It is worth noting that when working with the Reuters dataset, the train/test split ration of 33/66 was chosen to avoid getting such errors in the metrics: 'ValueError: The least populated class in y has only 1 member, which is too few'. This suggestion was taken from the following github page: https://github.com/davidsbatista/text-classification/issues/1. \n",
        "\n",
        "The base model of the Random Forest Classifier yieled an accuracy of 51% for both datasets. The hyperparameter grid search improved the accuracy for the Newsgroup (NG)dataset by 17% and for the Reuters dataset by 13%. The base model of the LinearSVC yieled an accuracy of 77% and of 81% for the NG dataset and the Reuters dataset respectively. The gridsearch had no signifcant on the first dataset (an increase of 0.004%) whereas in increased the accuracy by 4% for the Reuters Dataset.\n",
        "\n",
        "After comparing the SVC and LinearSVC on the newsgroup dataset (on a different notebook), the latter was chosen given how slow the former is train on such large datasets. However, using the the SVC with a non-linear kernel, intuitively and based upon my limited general knowledge of the machine learning, would have yieled better results due to the nature of the classification problem. The non-linear kernel function would generally yield better classification when a linear seperation cannot be found. \n",
        "\n",
        "Encoding and tokenzing (the reuters text) in a more detailed and efficient manner would have been another possible way to improve the results. Any possible comments/feedback on such matter would be greatly appreciated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4T6lgXbtyNs"
      },
      "source": [
        "# General Comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzE8CDqltzTP"
      },
      "source": [
        "All in all, this assignment served well to experiment with different types of encoding and models. Personally, I would really like to dig deeper into what types of pre-processing, encoding and models would work best with specific datasets. I think being adapative in the field of a machine learning is one important skill to have, one that I really hope to acquire in the future. "
      ]
    }
  ]
}